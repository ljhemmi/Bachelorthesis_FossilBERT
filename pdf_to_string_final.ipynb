{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pdf to text script\n",
    "*goal: use OCR edited pdf files and extract individual sentences & paragraphs to a table*\n",
    "\n",
    "This notebook is used to transform readable pdf documents into dataframes where each row either represents a sentence or a paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # function to remove unicode words like: https://github.com/traviscoan/cards/blob/master/preprocess.py\n",
    "# # maybe not needed\n",
    "# \n",
    "# def raw_text_cleaning(text):\n",
    "#     ### Light\n",
    "#     text = text.lower() # lowercase everything\n",
    "#     text = text.encode('ascii', 'ignore').decode()  # remove unicode characters\n",
    "#     text = re.sub(r'https*\\S+', ' ', text) # remove links\n",
    "#     text = re.sub(r'http*\\S+', ' ', text)\n",
    "#     \n",
    "#     # cleaning up text\n",
    "#     text = re.sub(r'\\'\\w+', '', text) \n",
    "#     text = re.sub(r'\\w*\\d+\\w*', '', text)\n",
    "#     text = re.sub(r'\\s{2,}', ' ', text)\n",
    "#     text = re.sub(r'\\s[^\\w\\s]\\s', '', text)\n",
    "# \n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## big loop for pdfs\n",
    "\n",
    "# import relevant libraries\n",
    "\n",
    "import tika\n",
    "from tika import parser # pip install tika\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('brown')\n",
    "\n",
    "\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# setup tika server instance\n",
    "tika.initVM()\n",
    "\n",
    "# define directory and extract file names\n",
    "dir_path = \"pdf/\"\n",
    "filenames = os.listdir(dir_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://github.com/chrismattmann/tika-python/issues/191\n",
    "#from io import StringIO\n",
    "#from bs4 import BeautifulSoup\n",
    "#from tika import parser\n",
    "\n",
    "\n",
    "\n",
    "#file_data = []\n",
    "#_buffer = StringIO()\n",
    "#data = parser.from_file(dir_path+filenames[name], xmlContent= True)\n",
    "#xhtml_data = BeautifulSoup(data['content'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1972-1-24_Mobil-nyt-66bn-mistake.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 20:48:55,012 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server-standard/2.6.0/tika-server-standard-2.6.0.jar to C:\\Users\\lucas\\AppData\\Local\\Temp\\tika-server.jar.\n",
      "2023-05-08 20:50:21,282 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server-standard/2.6.0/tika-server-standard-2.6.0.jar.md5 to C:\\Users\\lucas\\AppData\\Local\\Temp\\tika-server.jar.md5.\n",
      "2023-05-08 20:50:22,360 [MainThread  ] [WARNI]  Failed to see startup log message; retrying...\n",
      "2023-05-08 20:50:27,366 [MainThread  ] [WARNI]  Failed to see startup log message; retrying...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1981-2-12-Mobil-nyt-endcoalnightmare.pdf\n",
      "1981-7-30-Mobil-nyt-healthairstds.pdf\n",
      "1981-8-13-Mobil-nyt-costofcleanair.pdf\n",
      "1981-8-6-Mobil-nyt-canyonising.pdf\n",
      "1982-3-26-Mobil-nyt-caa.pdf\n",
      "1984-8-1-Mobil-nyt-Lies-They-Tell-Our-Children.pdf\n",
      "1986-10-21-Mobil-lat-taxesmisstarget.pdf\n",
      "1987-5-7-Mobil-nyt-rushtojudge.pdf\n",
      "1989-7-6-Mobil-nyt-peoplewholivegreenhouses.pdf\n",
      "1990-12-13-Mobil-nyt-cowsbullscleanair.pdf\n",
      "1990-9-27-Mobil-nyt-deliveringaspromised.pdf\n",
      "1992-4-2-Mobil-nyt-sensefmnonsense.pdf\n",
      "1993-12-9-Mobil-nyt-envtorpolitics.pdf\n",
      "1993-2-18-Mobil-nyt-whenthegoaliscleanair.pdf\n",
      "1993-2-25-Mobil-nyt-apocalypse-no.pdf\n",
      "1994-10-27-Mobil-nyt-whoindriversseat.pdf\n",
      "1995-9-28-Mobil-nyt-skynotfalling.pdf\n",
      "1996-12-12-Mobil-nyt-policyfortomorrow.pdf\n",
      "1996-12-5-Mobil-nyt-electriccars.pdf\n",
      "1996-7-18-Mobil-nyt-lessheatmorelight.pdf\n",
      "1996-8-1-Mobil-nyt-allintogether.pdf\n",
      "1997-10-16-Mobil-nyt-cnnslam.pdf\n",
      "1997-10-23-Mobil-nyt-globalclimatechange.pdf\n",
      "1997-10-30-Mobil-nyt-resetalarm.pdf\n",
      "1997-11-13-Mobil-nyt-climateprudentapproach.pdf\n",
      "1997-11-20-Mobil-nyt-ccwherewecomeout.pdf\n",
      "1997-11-6-Mobil-nyt-whatweknow.pdf\n",
      "1997-12-18-Mobil-nyt-kyotoconf.pdf\n",
      "1997-12-4-Mobil-nyt-uncertainty.pdf\n",
      "1997-3-6-Mobil-nyt-stoplooklisten.pdf\n",
      "1997-6-23-Mobil-nyt-letsgetitright.pdf\n",
      "1998-1-29-Mobil-nyt-postkyoto.pdf\n",
      "1998-11-5-Mobil-nyt-kyotopainfulresponse.pdf\n",
      "1999-12-9-2-XOM-nyt-tmrw-energy.pdf\n",
      "1999-4-15-Mobil-nyt-tncgreenwash.pdf\n",
      "1999-8-12-Mobil-nyt-c02-3-stabilisationscenarios.pdf\n",
      "1999-8-19-Mobil-nyt-c02-4-wehavetime.pdf\n",
      "1999-8-5-Mobil-nyt-c02-2-waystomakediff.pdf\n",
      "2000-10-12-XOM-nyt-en-envdeveloping.pdf\n",
      "2000-10-28-XOM-unbalanced-caution.pdf\n",
      "2000-10-5-XOM-nyt-globalization.pdf\n",
      "2000-12-7-XOM-nyt-conservation.pdf\n",
      "2000-3-16-XOM-do-no-harm.pdf\n",
      "2000-3-23-XOM-nyt-unsettledscience.pdf\n",
      "2000-3-30-XOM-nyt-techpromisecc.pdf\n",
      "2000-4-20-XOM-nyt-earthday.pdf\n",
      "2000-8-3-XOM-nyt-poweringeconomy.pdf\n",
      "2001-1-17-XOM-policy-for-new-admin.pdf\n",
      "2001-2-8-XOM-nyt-anwr.pdf\n",
      "2001-3-29-XOM-nyt-valdez12yrs.pdf\n",
      "2001-4-10-XOM-moving-past-kyoto.pdf\n",
      "2001-4-17-XOM-nyt-sounderclimatepolicy.pdf\n",
      "2001-7-19-XOM-nyt-cogenclimate.pdf\n",
      "2001-7-26-XOM-nyt-saudigas.pdf\n",
      "2001-8-2-XOM-nyt-siftingwinnowing.pdf\n",
      "2002-1-24-XOM-nyt-improveenergyuse.pdf\n",
      "2002-10-3-XOM-nyt-mgingghgemissions.pdf\n",
      "2002-11-22-XOM-nyt-gcep.pdf\n",
      "2002-5-2-XOM-nyt-hydrogenfuel.pdf\n",
      "2003-1-23-XOM-nyt-fracking.pdf\n",
      "2003-10-16-XOM-nyt-natgasnpc.pdf\n",
      "2003-10-2-XOM-nyt-natgasbalance.pdf\n",
      "2003-2-20-XOM-nyt-winwinenergysavings.pdf\n",
      "2003-2-3-XOM-nyt-globalclimategcep.pdf\n",
      "2003-6-17-XOM-nyt-bldgenergyfuture.pdf\n",
      "2003-7-10-XOM-nyt-deepwater.pdf\n",
      "2003-8-21-XOM-nyt-energybeyond2020.pdf\n",
      "2003-8-7-XOM-nyt-energyefficiency.pdf\n",
      "2004-1-21-XOM-climate-research-directions.pdf\n",
      "2004-1-21-XOM-weather-and-climate.pdf\n",
      "Mobil-Ads-From-Op-Ed-Impact-Study.pdf\n"
     ]
    }
   ],
   "source": [
    "# setup database for ExxonMobil pdf files\n",
    "# MAYBE try and optimize the code to not append to a .pd dataframe but rather a list object instead\n",
    "column_names = {'original_text':    ['original text'],\n",
    "                'cleaned_text':     ['cleaned text'],\n",
    "                'climate_related':  ['0/1'],\n",
    "                'downplaying':      ['0/1'],\n",
    "                'source':           ['filename'],\n",
    "                'type':             ['sentence/paragraph'],\n",
    "                'year':             [2000]}\n",
    "\n",
    "database_pdfs_exxon = pd.DataFrame(data = column_names)\n",
    "\n",
    "\n",
    "database_pdfs_exxon_paragraphs = pd.DataFrame(data = column_names)\n",
    "database_pdfs_exxon_paragraphs.drop(['original_text'], axis = 1, inplace = True)\n",
    "\n",
    "#paragraph_index = 0 # for paragraph selection internal loop\n",
    "\n",
    "\n",
    "for name in range(len(filenames)):\n",
    "\n",
    "    print(filenames[name])\n",
    "\n",
    "    # read the pdf from the corresponding file path\n",
    "    filename = filenames[name]\n",
    "    current_parsed = parser.from_file(dir_path+filenames[name], xmlContent= False)\n",
    "    current_raw_text = current_parsed['content']\n",
    "\n",
    "    blob_original_text = TextBlob(current_raw_text)\n",
    "\n",
    "    # cleaning the sentences\n",
    "    cleaned_full_text = re.sub(r'[.]\\n\\n', '. 777 ',current_raw_text) #looks for '.' followed by 'new line' to detect new paragraphs\n",
    "    cleaned_full_text = re.sub(r'[7][7]\\s\\s','88',cleaned_full_text)\n",
    "    cleaned_full_text = re.sub(r'\\n\\n',' ',cleaned_full_text)\n",
    "    cleaned_full_text = re.sub(r'-\\s','',cleaned_full_text)\n",
    "    cleaned_full_text = re.sub(r'-\\n','',cleaned_full_text)\n",
    "    cleaned_full_text = re.sub(r'\\n', ' ',cleaned_full_text)\n",
    "    cleaned_full_text = re.sub(r'\\s{2,}','',cleaned_full_text) # removes the whitespace infront of the starting sentence\n",
    "    cleaned_full_text = re.sub(r'[â€¢]\\s', '', cleaned_full_text) # removes bulletpoints\n",
    "\n",
    "    blob_cleaned_full_text = TextBlob(cleaned_full_text)\n",
    "    \n",
    "    current_paragraph = \"\"\n",
    "    \n",
    "\n",
    "    for i in range(len(blob_cleaned_full_text.sentences)):\n",
    "        \n",
    "        current_sentence = str(blob_cleaned_full_text.sentences[i])\n",
    "\n",
    "        if re.search(r'\\d{4}',filenames[name]):\n",
    "            current_year = int(re.findall(r'\\d{4}',filenames[name])[0])\n",
    "        else:\n",
    "            current_year = ''\n",
    "        \n",
    "        # check for missmatch with the original sentence data frame\n",
    "        if i+1 < len(blob_cleaned_full_text.sentences):\n",
    "            original_sentence = str(blob_original_text.sentences[i])\n",
    "        else:\n",
    "            original_sentence = \"NA\"\n",
    "        \n",
    "\n",
    "        # detecting paragraphs through special symbol\n",
    "        if re.search(r'777\\s',current_sentence):\n",
    "            current_type = 'start of paragraph'\n",
    "            \n",
    "            #paragraph_index = paragraph_index + 1\n",
    "            new_paragraph_row = [current_paragraph, 1, 1, filename, 'paragraph',current_year]\n",
    "            database_pdfs_exxon_paragraphs.loc[len(database_pdfs_exxon_paragraphs)] = new_paragraph_row\n",
    "\n",
    "\n",
    "\n",
    "            current_sentence = re.sub(r'^777\\s','',current_sentence)\n",
    "            current_paragraph = current_sentence\n",
    "            \n",
    "\n",
    "        elif re.search(r'788\\s[A-Z]',current_sentence):\n",
    "            current_type ='end of page (special case)'\n",
    "            \n",
    "            new_paragraph_row = [current_paragraph, 1, 1, filename, 'paragraph',current_year]\n",
    "            database_pdfs_exxon_paragraphs.loc[len(database_pdfs_exxon_paragraphs)] = new_paragraph_row\n",
    "            \n",
    "\n",
    "            current_sentence = re.sub(r'^788\\s','',current_sentence)\n",
    "            current_paragraph = current_sentence\n",
    "\n",
    "\n",
    "        elif re.search('788',current_sentence):\n",
    "            current_type = 'end of page'\n",
    "            \n",
    "            new_paragraph_row = [current_paragraph, 1, 1, filename, 'paragraph',current_year]\n",
    "            database_pdfs_exxon_paragraphs.loc[len(database_pdfs_exxon_paragraphs)] = new_paragraph_row\n",
    "            \n",
    "            current_sentence = re.sub(r'^788','NaN',current_sentence)\n",
    "            current_paragraph = ''\n",
    "\n",
    "        \n",
    "        else:\n",
    "            current_type = 'sentence'\n",
    "            current_paragraph = current_paragraph + ' ' + current_sentence\n",
    "\n",
    "\n",
    "        new_row = [original_sentence,current_sentence,1,1,filename,current_type,current_year]\n",
    "        database_pdfs_exxon.loc[len(database_pdfs_exxon)] = new_row\n",
    "\n",
    "\n",
    "database_pdfs_exxon['year'].replace({'':np.nan}, inplace = True)\n",
    "database_pdfs_exxon_paragraphs['year'].replace({'':np.nan}, inplace = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### same loop but for IPCC pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different folder selector\n",
    "dir_path_ipcc = \"ipcc/\"\n",
    "filenames_ipcc = os.listdir(dir_path_ipcc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1995_IPCC_SR_AR2_Summary_Policymakers.pdf\n",
      "2001_IPCC_SR_AR3_Total_Summary_Policymakers.pdf\n",
      "2001_IPCC_SR_AR3_wg1_Summary_Policymakers.pdf\n",
      "2001_IPCC_SR_AR3_wg2_Summary_Policymakers.pdf\n",
      "2001_IPCC_SR_AR3_wg3_Summary_Policymakers.pdf\n",
      "2007_IPCC_SR_AR4_Summary_Policymakers.pdf\n",
      "2014_IPCC_SR_AR5_Headlines.pdf\n",
      "2014_IPCC_SR_AR5_Summary_Policymakers.pdf\n",
      "2022_IPCC_SR_AR6_WGIII_Headlines.pdf\n",
      "2022_IPCC_SR_AR6_WGIII_Summary_Policymakers.pdf\n",
      "2022_IPCC_SR_AR6_WGII_Headlines.pdf\n",
      "2022_IPCC_SR_AR6_WGII_Summary_Policymakers.pdf\n"
     ]
    }
   ],
   "source": [
    "column_names = {'original_text':    ['original text'],\n",
    "                'cleaned_text':     ['cleaned text'],\n",
    "                'climate-related':  ['0/1'],\n",
    "                'downplaying':      ['0/1'],\n",
    "                'source':           ['filename'],\n",
    "                'type':             ['sentence/paragraph'],\n",
    "                'year':             ['2000']}\n",
    "\n",
    "database_pdfs_ipcc = pd.DataFrame(data = column_names)\n",
    "\n",
    "\n",
    "database_pdfs_ipcc_paragraphs = pd.DataFrame(data = column_names)\n",
    "database_pdfs_ipcc_paragraphs.drop(['original_text'], axis = 1, inplace = True)\n",
    "\n",
    "#paragraph_index = 0 # for paragraph selection internal loop\n",
    "\n",
    "\n",
    "for name in range(len(filenames_ipcc)):\n",
    "\n",
    "    print(filenames_ipcc[name])\n",
    "\n",
    "    # read the pdf from the corresponding file path\n",
    "    filename = filenames_ipcc[name]\n",
    "    current_parsed = parser.from_file(dir_path_ipcc+filenames_ipcc[name])\n",
    "    current_raw_text = current_parsed['content']\n",
    "\n",
    "    blob_original_text = TextBlob(current_raw_text)\n",
    "\n",
    "    # cleaning the sentences\n",
    "    # cleaned_full_text = re.sub(r'[.]\\n\\n', '. 777 ',current_raw_text) #looks for '.' followed by 'new line' to detect new paragraphs\n",
    "    # cleaned_full_text = re.sub(r'[7][7]\\s\\s','88',current_raw_text)\n",
    "    # cleaned_full_text = re.sub(r'(\\n\\n)',' 777 ',current_raw_text) #maybe need to re-add the white space\n",
    "    cleaned_full_text = re.sub(r'(\\n\\n)',' ',current_raw_text)\n",
    "    cleaned_full_text = re.sub(r'-\\n','',cleaned_full_text)\n",
    "    cleaned_full_text = re.sub(r'\\n', ' ',cleaned_full_text)\n",
    "    cleaned_full_text = re.sub(r'\\s{2,}',' ',cleaned_full_text) # removes the whitespace infront of the starting sentence\n",
    "    cleaned_full_text = re.sub(r'-\\s','5',cleaned_full_text)\n",
    "    cleaned_full_text = re.sub(r'[â€¢]\\s', '', cleaned_full_text) # removes bulletpoints\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # special cleaning for IPCC \"Language\", e.g. {references in brackets}\n",
    "    cleaned_full_text = re.sub(r'SPM.','',cleaned_full_text) #solves problem that TextBlob detects this \".\" as end of phrase\n",
    "    cleaned_full_text = re.sub(r'\\s\\((.*?)\\)','',cleaned_full_text)\n",
    "    cleaned_full_text = re.sub(r'\\[(.*?)\\]\\s', '', cleaned_full_text)\n",
    "    cleaned_full_text = re.sub(r'CO,\\s','CO2 ', cleaned_full_text)\n",
    "    cleaned_full_text = re.sub(r'\\{(.){3,}\\},', '', cleaned_full_text)\n",
    "    cleaned_full_text = re.sub(r'\\{(.*?)\\}','', cleaned_full_text)\n",
    "    cleaned_full_text = re.sub(r'[ï¿½]', '', cleaned_full_text)\n",
    "    cleaned_full_text = re.sub(r'\\xa0', ' ', cleaned_full_text)\n",
    "\n",
    "\n",
    "\n",
    "    blob_cleaned_full_text = TextBlob(cleaned_full_text)\n",
    "    \n",
    "    current_paragraph = \"\"\n",
    "    \n",
    "\n",
    "    for i in range(len(blob_cleaned_full_text.sentences)):\n",
    "        \n",
    "        current_sentence = str(blob_cleaned_full_text.sentences[i])\n",
    "\n",
    "        if re.search(r'\\d{4}',filenames_ipcc[name]):\n",
    "            current_year = int(re.findall(r'\\d{4}',filenames_ipcc[name])[0])\n",
    "        else:\n",
    "            current_year = ''\n",
    "\n",
    "\n",
    "        # check for missmatch with the original sentence data frame\n",
    "        if i+1 < len(blob_cleaned_full_text.sentences):\n",
    "            original_sentence = str(blob_original_text.sentences[i])\n",
    "        else:\n",
    "            original_sentence = \"NA\"\n",
    "        \n",
    "\n",
    "       # # detecting paragraphs through special symbol\n",
    "       # if re.search(r'^777\\s',current_sentence):\n",
    "       #     current_type = 'start of paragraph'\n",
    "       #     \n",
    "       #     #paragraph_index = paragraph_index + 1\n",
    "       #     new_paragraph_row = [current_paragraph, 1, 0, filename, 'paragraph',current_year]\n",
    "       #     database_pdfs_ipcc_paragraphs.loc[len(database_pdfs_ipcc_paragraphs)] = new_paragraph_row\n",
    "\n",
    "\n",
    "\n",
    "       #     current_sentence = re.sub(r'^777\\s','',current_sentence)\n",
    "       #     current_paragraph = current_sentence\n",
    "       #     \n",
    "\n",
    "       # elif re.search(r'788\\s[A-Z]',current_sentence):\n",
    "       #     current_type ='end of page (special case)'\n",
    "       #     \n",
    "       #     new_paragraph_row = [current_paragraph, 1, 0, filename, 'paragraph',current_year]\n",
    "       #     database_pdfs_ipcc_paragraphs.loc[len(database_pdfs_ipcc_paragraphs)] = new_paragraph_row\n",
    "       #     \n",
    "\n",
    "       #     current_sentence = re.sub(r'^788\\s','',current_sentence)\n",
    "       #     current_paragraph = current_sentence\n",
    "\n",
    "\n",
    "       # elif re.search('788',current_sentence):\n",
    "       #     current_type = 'end of page'\n",
    "       #     \n",
    "       #     new_paragraph_row = [current_paragraph, 1, 0, filename, 'paragraph',current_year]\n",
    "       #     database_pdfs_ipcc_paragraphs.loc[len(database_pdfs_ipcc_paragraphs)] = new_paragraph_row\n",
    "       #     \n",
    "       #     current_sentence = re.sub(r'^788','-',current_sentence)\n",
    "       #     current_paragraph = ''\n",
    "\n",
    "       # \n",
    "       # else:\n",
    "       #     current_type = 'sentence'\n",
    "       #     current_paragraph = current_paragraph + ' ' + current_sentence\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        new_row = [original_sentence,current_sentence,1,0,filename,current_type,current_year]\n",
    "        database_pdfs_ipcc.loc[len(database_pdfs_ipcc)] = new_row\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scraping of Greenpeace data  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "websites to scrape:\n",
    "- https://www.greenpeace.org/usa/issues/a-world-beyond-fossil-fuels/\n",
    "- https://www.greenpeace.org/usa/research/8-reasons-why-we-need-to-phase-out-the-fossil-fuel-industry/\n",
    "- https://www.greenpeace.org/usa/issues/climate-change-impacts/\n",
    "- https://www.greenpeace.org/usa/issues/california-climate-emergency/\n",
    "- https://www.greenpeace.org/usa/issues/climate-leadership/\n",
    "- https://www.greenpeace.org/usa/issues/renewable-energy/\n",
    "- https://www.greenpeace.org/usa/campaigns/climate/\n",
    "- https://www.greenpeace.org/usa/the-green-new-deal/\n",
    "- https://www.greenpeace.org/usa/fighting-climate-chaos/issues/\n",
    "- https://www.greenpeace.org/usa/fighting-climate-chaos/issues/coal/\n",
    "- https://www.greenpeace.org/usa/fighting-climate-chaos/issues/oil/\n",
    "- https://www.greenpeace.org/usa/fighting-climate-chaos/issues/fracking/\n",
    "- https://www.greenpeace.org/usa/fighting-climate-chaos/issues/natural-gas/\n",
    "- https://www.greenpeace.org/usa/fighting-climate-chaos/\n",
    "- https://www.greenpeace.org/usa/fighting-climate-chaos/climate-science/\n",
    "- https://www.greenpeace.org/usa/fighting-climate-chaos/renewable-energy-future/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "greenpeace_websites = [ \"https://www.greenpeace.org/usa/issues/a-world-beyond-fossil-fuels/\",\n",
    "                        \"https://www.greenpeace.org/usa/research/8-reasons-why-we-need-to-phase-out-the-fossil-fuel-industry/\",\n",
    "                        \"https://www.greenpeace.org/usa/issues/climate-change-impacts/\",\n",
    "                        \"https://www.greenpeace.org/usa/issues/california-climate-emergency/\",\n",
    "                        \"https://www.greenpeace.org/usa/issues/climate-leadership/\",\n",
    "                        \"https://www.greenpeace.org/usa/issues/renewable-energy/\",\n",
    "                        \"https://www.greenpeace.org/usa/campaigns/climate/\",\n",
    "                        \"https://www.greenpeace.org/usa/the-green-new-deal/\",\n",
    "                        \"https://www.greenpeace.org/usa/fighting-climate-chaos/issues/\",\n",
    "                        \"https://www.greenpeace.org/usa/fighting-climate-chaos/issues/coal/\",\n",
    "                        \"https://www.greenpeace.org/usa/fighting-climate-chaos/issues/oil/\",\n",
    "                        \"https://www.greenpeace.org/usa/fighting-climate-chaos/issues/fracking/\",\n",
    "                        \"https://www.greenpeace.org/usa/fighting-climate-chaos/issues/natural-gas/\",\n",
    "                        \"https://www.greenpeace.org/usa/fighting-climate-chaos/\",\n",
    "                        \"https://www.greenpeace.org/usa/fighting-climate-chaos/climate-science/\",\n",
    "                        \"https://www.greenpeace.org/usa/fighting-climate-chaos/renewable-energy-future/\",\n",
    "                        \"https://www.greenpeace.org/usa/big-oil-forces-a-hold-on-california-law-ending-neighborhood-drilling/\",\n",
    "                        \"https://www.greenpeace.org/usa/bidens-alarming-choice-to-allow-the-largest-sale-of-public-waters-for-offshore-oil-drilling-in-us-history/\",\n",
    "                        \"https://www.greenpeace.org/usa/top-5-truly-scary-facts-about-our-climate-crisis/\",\n",
    "                        \"https://www.greenpeace.org/usa/no-climate-no-deal-how-progressives-are-holding-the-line-for-the-full-build-back-better-act/\",\n",
    "                        \"https://www.greenpeace.org/usa/global-heatwaves-are-fossil-fuel-driven-climate-chaos/\",\n",
    "                        \"https://www.greenpeace.org/usa/why-net-zero-and-offsets-wont-solve-the-climate-crisis/\",\n",
    "                        \"https://www.greenpeace.org/usa/joe-biden-marks-the-beginning-of-the-end-for-the-oil-industry/\",\n",
    "                        \"https://www.greenpeace.org/usa/4-ways-to-shrink-the-fossil-fuel-industry-and-protect-the-climate/\",\n",
    "                        \"https://www.greenpeace.org/usa/8-reasons-why-we-need-to-phase-out-the-fossil-fuel-industry/\",\n",
    "                        \"https://www.greenpeace.org/usa/if-we-dont-stop-producing-fossil-fuels-we-wont-make-it/\",\n",
    "                        \"https://www.greenpeace.org/usa/lets-talk-about-climate-change/\",\n",
    "                        \"https://www.greenpeace.org/usa/heres-everything-wrong-with-scott-pruitts-plan-to-debate-climate-science/\",\n",
    "                        \"https://www.greenpeace.org/usa/trumps-attack-on-the-paris-climate-agreement-has-officially-begun/\",\n",
    "                        \"https://www.greenpeace.org/usa/4-actually-true-facts-about-jobs-renewable-energy-and-fossil-fuels/\",\n",
    "                        \"https://www.greenpeace.org/usa/trumps-latest-executive-order-is-an-all-out-attack-on-clean-energy/\",\n",
    "                        \"https://www.greenpeace.org/usa/trumps-refusal-to-act-on-climate-could-cost-trillions-of-dollars-every-year-for-decades/\"\n",
    "                        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.greenpeace.org/usa/issues/a-world-beyond-fossil-fuels/\n",
      "https://www.greenpeace.org/usa/research/8-reasons-why-we-need-to-phase-out-the-fossil-fuel-industry/\n",
      "https://www.greenpeace.org/usa/issues/climate-change-impacts/\n",
      "https://www.greenpeace.org/usa/issues/california-climate-emergency/\n",
      "https://www.greenpeace.org/usa/issues/climate-leadership/\n",
      "https://www.greenpeace.org/usa/issues/renewable-energy/\n",
      "https://www.greenpeace.org/usa/campaigns/climate/\n",
      "https://www.greenpeace.org/usa/the-green-new-deal/\n",
      "https://www.greenpeace.org/usa/fighting-climate-chaos/issues/\n",
      "https://www.greenpeace.org/usa/fighting-climate-chaos/issues/coal/\n",
      "https://www.greenpeace.org/usa/fighting-climate-chaos/issues/oil/\n",
      "https://www.greenpeace.org/usa/fighting-climate-chaos/issues/fracking/\n",
      "https://www.greenpeace.org/usa/fighting-climate-chaos/issues/natural-gas/\n",
      "https://www.greenpeace.org/usa/fighting-climate-chaos/\n",
      "https://www.greenpeace.org/usa/fighting-climate-chaos/climate-science/\n",
      "https://www.greenpeace.org/usa/fighting-climate-chaos/renewable-energy-future/\n",
      "https://www.greenpeace.org/usa/big-oil-forces-a-hold-on-california-law-ending-neighborhood-drilling/\n",
      "https://www.greenpeace.org/usa/bidens-alarming-choice-to-allow-the-largest-sale-of-public-waters-for-offshore-oil-drilling-in-us-history/\n",
      "https://www.greenpeace.org/usa/top-5-truly-scary-facts-about-our-climate-crisis/\n",
      "https://www.greenpeace.org/usa/no-climate-no-deal-how-progressives-are-holding-the-line-for-the-full-build-back-better-act/\n",
      "https://www.greenpeace.org/usa/global-heatwaves-are-fossil-fuel-driven-climate-chaos/\n",
      "https://www.greenpeace.org/usa/why-net-zero-and-offsets-wont-solve-the-climate-crisis/\n",
      "https://www.greenpeace.org/usa/joe-biden-marks-the-beginning-of-the-end-for-the-oil-industry/\n",
      "https://www.greenpeace.org/usa/4-ways-to-shrink-the-fossil-fuel-industry-and-protect-the-climate/\n",
      "https://www.greenpeace.org/usa/8-reasons-why-we-need-to-phase-out-the-fossil-fuel-industry/\n",
      "https://www.greenpeace.org/usa/if-we-dont-stop-producing-fossil-fuels-we-wont-make-it/\n",
      "https://www.greenpeace.org/usa/lets-talk-about-climate-change/\n",
      "https://www.greenpeace.org/usa/heres-everything-wrong-with-scott-pruitts-plan-to-debate-climate-science/\n",
      "https://www.greenpeace.org/usa/trumps-attack-on-the-paris-climate-agreement-has-officially-begun/\n",
      "https://www.greenpeace.org/usa/4-actually-true-facts-about-jobs-renewable-energy-and-fossil-fuels/\n",
      "https://www.greenpeace.org/usa/trumps-latest-executive-order-is-an-all-out-attack-on-clean-energy/\n",
      "https://www.greenpeace.org/usa/trumps-refusal-to-act-on-climate-could-cost-trillions-of-dollars-every-year-for-decades/\n"
     ]
    }
   ],
   "source": [
    "column_names = {'original_text':    ['original text'],\n",
    "                'cleaned_text':     ['cleaned text'],\n",
    "                'climate-related':  ['0/1'],\n",
    "                'downplaying':      ['0/1'],\n",
    "                'source':           ['filename'],\n",
    "                'type':             ['sentence/paragraph'],\n",
    "                'year':             ['2000']}\n",
    "\n",
    "database_pdfs_greenpeace = pd.DataFrame(data = column_names)\n",
    "\n",
    "\n",
    "\n",
    "#paragraph_index = 0 # for paragraph selection internal loop\n",
    "\n",
    "\n",
    "for website in range(len(greenpeace_websites)):\n",
    "\n",
    "    print(greenpeace_websites[website])\n",
    "\n",
    "\n",
    "    website_name = greenpeace_websites[website]\n",
    "\n",
    "    # read the html from the corresponding url\n",
    "    url = greenpeace_websites[website]\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    filtered_for_class = soup.find_all(\"div\", class_=[\"articleBody\",\"primary-content\", \"post-content\"])\n",
    "\n",
    "    current_raw_text = filtered_for_class[0].text\n",
    "\n",
    "\n",
    "    blob_original_text = TextBlob(current_raw_text)\n",
    "\n",
    "    # cleaning the sentences\n",
    "    cleaned_full_text = re.sub(r'(\\xa0\\n)',' ',current_raw_text)\n",
    "    cleaned_full_text = re.sub(r'(\\n\\n)','',cleaned_full_text)\n",
    "    cleaned_full_text = re.sub(r'\\n', ' ',cleaned_full_text)\n",
    "    cleaned_full_text = re.sub(r'\\xa0', ' ',cleaned_full_text)\n",
    "\n",
    "    \n",
    "\n",
    "    # special cleaning for greenpeace \"Language\", e.g. {references in brackets}\n",
    "   \n",
    "\n",
    "\n",
    "    blob_cleaned_full_text = TextBlob(cleaned_full_text)\n",
    "    \n",
    "    current_paragraph = \"\"\n",
    "    \n",
    "    \n",
    "    for i in range(len(blob_cleaned_full_text.sentences)):\n",
    "        \n",
    "        current_sentence = str(blob_cleaned_full_text.sentences[i])\n",
    "        original_sentence = str(blob_original_text.sentences[i])\n",
    "\n",
    "        # if re.search(r'\\d{4}',filenames_ipcc[name]):\n",
    "        #     current_year = int(re.findall(r'\\d{4}',filenames_ipcc[name])[0])\n",
    "        # else:\n",
    "        #     current_year = ''\n",
    "\n",
    "\n",
    "        new_row = [original_sentence,current_sentence,1,0,website_name,\"sentence\",\"XXXX\"]\n",
    "        database_pdfs_greenpeace.loc[len(database_pdfs_greenpeace)] = new_row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter greenpeace sentences that are too short\n",
    "database_pdfs_greenpeace['word_count'] = [len(sentence.split()) for sentence in database_pdfs_greenpeace['cleaned_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_pdfs_greenpeace_filtered = database_pdfs_greenpeace[database_pdfs_greenpeace['word_count'] >= 6]\n",
    "database_pdfs_greenpeace_filtered.reset_index(drop = True, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframes to .csv files\n",
    "database_pdfs_ipcc.to_csv('ipcc_out.csv')\n",
    "database_pdfs_exxon.to_csv('exxon_out.csv')\n",
    "database_pdfs_exxon_paragraphs.to_csv('exxon_paragraphs_out.csv')\n",
    "database_pdfs_greenpeace_filtered.to_csv('greenpeace_out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframes to .parquet files\n",
    "#database_pdfs_ipcc.to_parquet('ipcc_out.parquet')\n",
    "database_pdfs_exxon[['cleaned_text','source','type','year']].to_parquet('exxon_out.parquet')\n",
    "database_pdfs_exxon_paragraphs[['cleaned_text','source','type','year']].to_parquet('exxon_paragraphs_out.parquet')\n",
    "#database_pdfs_greenpeace_filtered.to_parquet('greenpeace_out.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "665796ea3363072d3a6057ac2fdbe3c4fcb0d17a4b92295d9707f78e9c46c0af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
