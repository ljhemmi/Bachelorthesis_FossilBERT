{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering Twitter Data - Tweets to raw text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*goal: download tweets from relevant accounts for training the model and evaluating it*\n",
    "\n",
    "This notebook is structured into the following sections:\n",
    "- 1.  collection of command line commands used to access the twitter api through twarc2 (for selected politicians where the entire twitter timeline was gathered)\n",
    "- 2.  reloading the gathered tweets and cleaning them for further analysis\n",
    "- 3.  loading the rehydrated politicians tweets from van Vliet's dataset\n",
    "- 4.  filtering for US, UK & Australian politicians\n",
    "- 5.  exporting the final dataframes to be inferred by climate classifiere and FossilBERT classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import relevant libraries\n",
    "\n",
    "import tika\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#twarc process\n",
    "from twarc import Twarc2, expansions\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# general libraries for cleaning and dealing with .csv files\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import wget\n",
    "\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#research account\n",
    "client = Twarc2(bearer_token=\"add your research account bearer token\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant Twarc2 commands\n",
    "### to be used within the command prompt (in Visual Studio Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#twarc is a command line tool, below are the commands to access the data:\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# twitter accounts for \"downplaying role of fossil fuels in climate change\" and command used:\n",
    "### - EnergyInDepth\n",
    "# twarc2 timeline --use-search EnergyInDepth EnergyInDepth_09012023_full_timeline.jsonl\n",
    "\n",
    "### - energycitizens\n",
    "# twarc2 timeline --use-search energycitizens energycitizens_09012023_full_timeline.jsonl\n",
    "\n",
    "### - APIenergy\n",
    "# twarc2 timeline --use-search APIenergy APIenergy_09012023_full_timeline.jsonl\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# twitter accounts for \"undescoring the role of fossil fuels in climate change\" and command used:\n",
    "### - IPCC_CH\n",
    "# twarc2 timeline --use-search IPCC_CH IPCC_CH_09012023_full_timeline.jsonl\n",
    "\n",
    "### - UNFCCC\n",
    "# twarc2 timeline --use-search UNFCCC UNFCCC_09012023_full_timeline.jsonl\n",
    "\n",
    "### - WBG_Climate\n",
    "# twarc2 timeline --use-search WBG_Climate WBG_Climate_09012023_full_timeline.jsonl\n",
    "\n",
    "### - greenpeaceusa\n",
    "# twarc2 timeline --use-search greenpeaceusa greenpeaceusa_22022023_full_timeline.jsonl\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# twitter accounts for \"political analysis\" and command used:\n",
    "### - algore\n",
    "# twarc2 timeline --use-search algore algore_09012023_full_timeline.jsonl\n",
    "\n",
    "### - BarackObama\n",
    "# twarc2 timeline --use-search BarackObama BarackObama_09012023_full_timeline.jsonl\n",
    "\n",
    "### - realDonaldTrump (NOT YET WORKING; MAYBE REFER TO https://www.thetrumparchive.com/ to download the .csv manually)\n",
    "# twarc2 timeline --use-search realDonaldTrump realDonaldTrump_09012023_full_timeline.jsonl\n",
    "\n",
    "### - JunkScience (Steve Milloy - lawyer, lobbyist, author and Fox News commentator)\n",
    "# twarc2 timeline --use-search JunkScience JunkScience_09012023_full_timeline.jsonl\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# twarc2 timeline --use-search BjornLomborg bjornlomborg_08042023_full_timeline.jsonl\n",
    "\n",
    "# twarc2 timeline --use-search MikeHudema mikehudema_08042023_full_timeline.jsonl    \n",
    "\n",
    "# twarc2 timeline --use-search TonyClimate tonyheller_08042023_full_timeline.jsonl\n",
    "\n",
    "# twarc2 timeline --use-search GeorgeMonbiot georgemonbiot_08042023_full_timeline.jsonl\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# twarc2 timeline --use-search AOC AOC_09042023_full_timeline.jsonl\n",
    "\n",
    "# twarc2 timeline --use-search tedcruz tedcruz_09042023_full_timeline.jsonl\n",
    " \n",
    "# twarc2 timeline --use-search RepDonBeyer RepDonBeyer_09042023_full_timeline.jsonl \n",
    "\n",
    "# twarc2 timeline --use-search SenJoniErnst SenJoniErnst_09042023_full_timeline.jsonl \n",
    "\n",
    "# twarc2 timeline --use-search SenSanders SenSanders_09042023_full_timeline.jsonl\n",
    "\n",
    "# twarc2 timeline --use-search RepGosar RepGosar_09042023_full_timeline.jsonl\n",
    "\n",
    "# twarc2 timeline --use-search SenJohnHoeven SenJohnHoeven_09042023_full_timeline.jsonl\n",
    "\n",
    "# twarc2 timeline --use-search MikeCrapo MikeCrapo_09042023_full_timeline.jsonl\n",
    "\n",
    "# twarc2 timeline --use-search MartinHeinrich MartinHeinrich_09042023_full_timeline.jsonl\n",
    "\n",
    "# twarc2 timeline --use-search RepScottPeters RepScottPeters_09042023_full_timeline.jsonl\n",
    "\n",
    "# twarc2 timeline --use-search SenKevinCramer SenKevinCramer_09042023_full_timeline.jsonl\n",
    "\n",
    "# twarc2 timeline --use-search WestermanAR WestermanAR_09042023_full_timeline.jsonl\n",
    "\n",
    "# twarc2 timeline --use-search RepMikeQuigley RepMikeQuigley_09042023_full_timeline.jsonl\n",
    "\n",
    "# twarc2 timeline --use-search RepLizCheney RepLizCheney_09042023_full_timeline.jsonl\n",
    "\n",
    "# twarc2 timeline --use-search Sen_JoeManchin Sen_JoeManchin_09042023_full_timeline.jsonl\n",
    "\n",
    "# twarc2 timeline --use-search SpeakerMcCarthy SpeakerMcCarthy_09042023_full_timeline.jsonl\n",
    "\n",
    "# twarc2 timeline --use-search RepCuellar RepCuellar_09042023_full_timeline.jsonl\n",
    "\n",
    "# twarc2 timeline --use-search RepPfluger RepPfluger_09042023_full_timeline.jsonl\n",
    "\n",
    "# twarc2 timeline --use-search SenatorLankford SenatorLankford_09042023_full_timeline.jsonl\n",
    "\n",
    "# twarc2 timeline --use-search WesleyHuntTX WesleyHuntTX_09042023_full_timeline.jsonl\n",
    "\n",
    "# twarc2 timeline --use-search RepFletcher RepFletcher_09042023_full_timeline.jsonl\n",
    "\n",
    "# twarc2 timeline --use-search RepDanCrenshaw RepDanCrenshaw_09042023_full_timeline.jsonl\n",
    "\n",
    "# twarc2 timeline --use-search SteveScalise SteveScalise__09042023_full_timeline.jsonl\n",
    "\n",
    "# twarc2 timeline --use-search RepPaulTonko RepPaulTonko__09042023_full_timeline.jsonl\n",
    "\n",
    "# twarc2 timeline --use-search BetoORourke BetoORourke__09042023_full_timeline.jsonl\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# twarc2 timeline --use-search davidcicilline davidcicilline_09042023_full_timeline.jsonl\n",
    "# twarc2 csv davidcicilline_09042023_full_timeline.jsonl davidcicilline_09042023.csv\n",
    "\n",
    "# twarc2 timeline --use-search Malinowski Malinowski_09042023_full_timeline.jsonl\n",
    "# twarc2 csv Malinowski_09042023_full_timeline.jsonl Malinowski_09042023.csv\n",
    "\n",
    "# twarc2 timeline --use-search RepAndyHarrisMD RepAndyHarrisMD_09042023_full_timeline.jsonl\n",
    "# twarc2 csv RepAndyHarrisMD_09042023_full_timeline.jsonl RepAndyHarrisMD_09042023.csv\n",
    "\n",
    "# twarc2 timeline --use-search EliseStefanik EliseStefanik_09042023_full_timeline.jsonl\n",
    "# twarc2 csv EliseStefanik_09042023_full_timeline.jsonl EliseStefanik_09042023.csv\n",
    "\n",
    "# twarc2 timeline --use-search RepJamesClyburn RepJamesClyburn_09042023_full_timeline.jsonl\n",
    "# twarc2 csv RepJamesClyburn_09042023_full_timeline.jsonl RepJamesClyburn_09042023.csv\n",
    "\n",
    "# twarc2 timeline --use-search RepMoolenaar RepMoolenaar_09042023_full_timeline.jsonl\n",
    "# twarc2 csv RepMoolenaar_09042023_full_timeline.jsonl RepMoolenaar_09042023.csv\n",
    "\n",
    "# twarc2 timeline --use-search RepSmucker RepSmucker_09042023_full_timeline.jsonl\n",
    "# twarc2 csv RepSmucker_09042023_full_timeline.jsonl RepSmucker_09042023.csv\n",
    "\n",
    "# twarc2 timeline --use-search RepRodBlum RepRodBlum_09042023_full_timeline.jsonl\n",
    "# twarc2 csv RepRodBlum_09042023_full_timeline.jsonl RepRodBlum_09042023.csv\n",
    "\n",
    "# twarc2 timeline --use-search StaceyPlaskett StaceyPlaskett_09042023_full_timeline.jsonl\n",
    "# twarc2 csv StaceyPlaskett_09042023_full_timeline.jsonl StaceyPlaskett_09042023.csv\n",
    "\n",
    "# twarc2 timeline --use-search RepHuizenga RepHuizenga_09042023_full_timeline.jsonl\n",
    "# twarc2 csv RepHuizenga_09042023_full_timeline.jsonl RepHuizenga_09042023.csv\n",
    "\n",
    "# twarc2 timeline --use-search RepAndyBarr RepAndyBarr_09042023_full_timeline.jsonl\n",
    "# twarc2 csv RepAndyBarr_09042023_full_timeline.jsonl RepAndyBarr_09042023.csv\n",
    "\n",
    "# twarc2 timeline --use-search RepCloudTX RepCloudTX_09042023_full_timeline.jsonl\n",
    "# twarc2 csv RepCloudTX_09042023_full_timeline.jsonl RepCloudTX_09042023.csv\n",
    "\n",
    "# twarc2 timeline --use-search RepBera RepBera_09042023_full_timeline.jsonl\n",
    "# twarc2 csv RepBera_09042023_full_timeline.jsonl RepBera_09042023.csv\n",
    "\n",
    "# twarc2 timeline --use-search RepBrendanBoyle RepBrendanBoyle_09042023_full_timeline.jsonl\n",
    "# twarc2 csv RepBrendanBoyle_09042023_full_timeline.jsonl RepBrendanBoyle_09042023.csv\n",
    "\n",
    "# twarc2 timeline --use-search RepDebDingell RepDebDingell_09042023_full_timeline.jsonl\n",
    "# twarc2 csv RepDebDingell_09042023_full_timeline.jsonl RepDebDingell_09042023.csv\n",
    "\n",
    "# twarc2 timeline --use-search RepWilson RepWilson_09042023_full_timeline.jsonl\n",
    "# twarc2 csv RepWilson_09042023_full_timeline.jsonl RepWilson_09042023.csv\n",
    "\n",
    "# twarc2 timeline --use-search RepJackBergman RepJackBergman_09042023_full_timeline.jsonl\n",
    "# twarc2 csv RepJackBergman_09042023_full_timeline.jsonl RepJackBergman_09042023.csv\n",
    "\n",
    "# twarc2 timeline --use-search RepSarbanes RepSarbanes_09042023_full_timeline.jsonl\n",
    "# twarc2 csv RepSarbanes_09042023_full_timeline.jsonl RepSarbanes_09042023.csv\n",
    "\n",
    "# twarc2 timeline --use-search RepJudyChu RepJudyChu_09042023_full_timeline.jsonl\n",
    "# twarc2 csv RepJudyChu_09042023_full_timeline.jsonl RepJudyChu_09042023.csv\n",
    "\n",
    "# twarc2 timeline --use-search SenatorSinema SenatorSinema_09042023_full_timeline.jsonl\n",
    "# twarc2 csv SenatorSinema_09042023_full_timeline.jsonl SenatorSinema_09042023.csv\n",
    "\n",
    "# twarc2 timeline --use-search SecFudge SecFudge_09042023_full_timeline.jsonl\n",
    "# twarc2 csv SecFudge_09042023_full_timeline.jsonl SecFudge_09042023.csv\n",
    "\n",
    "# twarc2 timeline --use-search RepCartwright RepCartwright_09042023_full_timeline.jsonl\n",
    "# twarc2 csv RepCartwright_09042023_full_timeline.jsonl RepCartwright_09042023.csv\n",
    "\n",
    "# twarc2 timeline --use-search PeteSessions PeteSessions_09042023_full_timeline.jsonl\n",
    "# twarc2 csv PeteSessions_09042023_full_timeline.jsonl PeteSessions_09042023.csv\n",
    "\n",
    "# twarc2 timeline --use-search RepRickAllen RepRickAllen_09042023_full_timeline.jsonl\n",
    "# twarc2 csv RepRickAllen_09042023_full_timeline.jsonl RepRickAllen_09042023.csv\n",
    "\n",
    "# twarc2 timeline --use-search RogerMarshallMD RogerMarshallMD_09042023_full_timeline.jsonl\n",
    "# twarc2 csv RogerMarshallMD_09042023_full_timeline.jsonl RogerMarshallMD_09042023.csv\n",
    "\n",
    "# twarc2 timeline --use-search sethmoulton sethmoulton_09042023_full_timeline.jsonl\n",
    "# twarc2 csv sethmoulton_09042023_full_timeline.jsonl sethmoulton_09042023.csv\n",
    "\n",
    "# twarc2 timeline --use-search RepTedDeutch RepTedDeutch_09042023_full_timeline.jsonl\n",
    "# twarc2 csv RepTedDeutch_09042023_full_timeline.jsonl RepTedDeutch_09042023.csv\n",
    "\n",
    "# twarc2 timeline --use-search RepTrentKelly RepTrentKelly_09042023_full_timeline.jsonl\n",
    "# twarc2 csv RepTrentKelly_09042023_full_timeline.jsonl RepTrentKelly_09042023.csv\n",
    "\n",
    "# twarc2 timeline --use-search USRepKeating USRepKeating_09042023_full_timeline.jsonl\n",
    "# twarc2 csv USRepKeating_09042023_full_timeline.jsonl USRepKeating_09042023.csv\n",
    "\n",
    "# twarc2 timeline --use-search RepRobinKelly RepRobinKelly_09042023_full_timeline.jsonl\n",
    "# twarc2 csv RepRobinKelly_09042023_full_timeline.jsonl RepRobinKelly_09042023.csv\n",
    "\n",
    "# twarc2 timeline --use-search CongressmanHice CongressmanHice_09042023_full_timeline.jsonl\n",
    "# twarc2 csv CongressmanHice_09042023_full_timeline.jsonl CongressmanHice_09042023.csv\n",
    "\n",
    "# twarc2 timeline --use-search RepFilemonVela RepFilemonVela_09042023_full_timeline.jsonl\n",
    "# twarc2 csv RepFilemonVela_09042023_full_timeline.jsonl RepFilemonVela_09042023.csv\n",
    "\n",
    "# twarc2 timeline --use-search USRepMikeDoyle USRepMikeDoyle_09042023_full_timeline.jsonl\n",
    "# twarc2 csv USRepMikeDoyle_09042023_full_timeline.jsonl USRepMikeDoyle_09042023.csv\n",
    "\n",
    "# twarc2 timeline --use-search WhipKClark WhipKClark_09042023_full_timeline.jsonl\n",
    "# twarc2 csv WhipKClark_09042023_full_timeline.jsonl WhipKClark_09042023.csv\n",
    "\n",
    "# twarc2 timeline --use-search JeffFortenberry JeffFortenberry_09042023_full_timeline.jsonl\n",
    "# twarc2 csv JeffFortenberry_09042023_full_timeline.jsonl JeffFortenberry_09042023.csv\n",
    "\n",
    "# twarc2 timeline --use-search RepSpeier RepSpeier_09042023_full_timeline.jsonl\n",
    "# twarc2 csv RepSpeier_09042023_full_timeline.jsonl RepSpeier_09042023.csv\n",
    "\n",
    "\n",
    "# Jim Banks: Republican, 1979\n",
    "# twarc2 timeline --use-search RepJimBanks RepJimBanks_09042023_full_timeline.jsonl\n",
    "# twarc2 csv RepJimBanks_09042023_full_timeline.jsonl RepJimBanks_09042023.csv\n",
    "\n",
    "\n",
    "# Rashida Tlaib: Democrat, 1976\n",
    "# twarc2 timeline --use-search RepRashida RepRashida_09042023_full_timeline.jsonl\n",
    "# twarc2 csv RepRashida_09042023_full_timeline.jsonl RepRashida_09042023.csv\n",
    "\n",
    "\n",
    "# alex mooney: Republican, 1971\n",
    "# twarc2 timeline --use-search RepAlexMooney RepAlexMooney_09042023_full_timeline.jsonl\n",
    "# twarc2 csv RepAlexMooney_09042023_full_timeline.jsonl RepAlexMooney_09042023.csv\n",
    "\n",
    "\n",
    "# cheri bustos: Democrat, 1961\n",
    "# twarc2 timeline --use-search RepCheri RepCheri_09042023_full_timeline.jsonl\n",
    "# twarc2 csv RepCheri_09042023_full_timeline.jsonl RepCheri_09042023.csv\n",
    "\n",
    "\n",
    "# hakeem jeffries: Democrat, 1970\n",
    "# twarc2 timeline --use-search RepJeffries RepJeffries_09042023_full_timeline.jsonl\n",
    "# twarc2 csv RepJeffries_09042023_full_timeline.jsonl RepJeffries_09042023.csv\n",
    "\n",
    "# RepAnnieKuster: Democrat, 1956\n",
    "# twarc2 timeline --use-search RepAnnieKuster RepAnnieKuster_09042023_full_timeline.jsonl\n",
    "# twarc2 csv RepAnnieKuster_09042023_full_timeline.jsonl RepAnnieKuster_09042023.csv\n",
    "\n",
    "# RepBillJohnson: Republican, 1954\n",
    "# twarc2 timeline --use-search RepBillJohnson RepBillJohnson_09042023_full_timeline.jsonl\n",
    "# twarc2 csv RepBillJohnson_09042023_full_timeline.jsonl RepBillJohnson_09042023.csv\n",
    "\n",
    "# RepBrianFitz: Republican, 1973\n",
    "# twarc2 timeline --use-search RepBrianFitz RepBrianFitz_09042023_full_timeline.jsonl\n",
    "# twarc2 csv RepBrianFitz_09042023_full_timeline.jsonl RepBrianFitz_09042023.csv\n",
    "\n",
    "# chelliepingree: Democrat, 1955\n",
    "# twarc2 timeline --use-search chelliepingree chelliepingree_09042023_full_timeline.jsonl\n",
    "# twarc2 csv chelliepingree_09042023_full_timeline.jsonl chelliepingree_09042023.csv\n",
    "\n",
    "# LtGovDennyHeck: Democrat, 1952\n",
    "# twarc2 timeline --use-search LtGovDennyHeck LtGovDennyHeck_09042023_full_timeline.jsonl\n",
    "# twarc2 csv LtGovDennyHeck_09042023_full_timeline.jsonl LtGovDennyHeck_09042023.csv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# conversion of .jsonl format to .csv\n",
    "\n",
    "## twarc2 csv tweets.jsonl tweets.csv \n",
    "\n",
    "# requires: pip3 install --upgrade twarc-csv\n",
    "\n",
    "\n",
    "# \"downplaying\" ---------------------\n",
    "\n",
    "### - EnergyInDepth\n",
    "# twarc2 csv EnergyInDepth_09012023_full_timeline.jsonl EnergyInDepth_09012023.csv\n",
    "\n",
    "### - energycitizens\n",
    "# twarc2 csv energycitizens_09012023_full_timeline.jsonl energycitizens_09012023.csv\n",
    "\n",
    "### - APIenergy\n",
    "# twarc2 csv APIenergy_09012023_full_timeline.jsonl APIenergy_09012023.csv\n",
    "\n",
    "\n",
    "# \"underscoring\" ---------------------\n",
    "\n",
    "### - IPCC_CH\n",
    "# twarc2 csv IPCC_CH_09012023_full_timeline.jsonl IPCC_CH_09012023.csv\n",
    "\n",
    "### - UNFCCC\n",
    "# twarc2 csv UNFCCC_09012023_full_timeline.jsonl UNFCCC_09012023.csv\n",
    "\n",
    "### - WBG_Climate\n",
    "# twarc2 csv WBG_Climate_09012023_full_timeline.jsonl WBG_Climate_09012023.csv\n",
    "\n",
    "### - greenpeaceusa\n",
    "# twarc2 csv greenpeaceusa_22022023_full_timeline.jsonl greenpeaceusa_22022023.csv\n",
    "\n",
    "\n",
    "\n",
    "# \"politcal analysis\" ---------------------\n",
    "\n",
    "### - algore\n",
    "# twarc2 csv algore_09012023_full_timeline.jsonl algore_09012023.csv\n",
    "\n",
    "### - BarackObama\n",
    "# twarc2 csv BarackObama_09012023_full_timeline.jsonl BarackObama_09012023.csv\n",
    "\n",
    "### - realDonaldTrump\n",
    "# twarc2 csv realDonaldTrump_09012023_full_timeline.jsonl realDonaldTrump_09012023.csv\n",
    "\n",
    "### - JunkScience\n",
    "# twarc2 csv JunkScience_09012023_full_timeline.jsonl JunkScience_09012023.csv\n",
    "\n",
    "\n",
    "## look at this: https://figshare.com/articles/dataset/The_Twitter_Parliamentarian_Database/10120685"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning of the Twitter Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean the tweet strings\n",
    "def tweet_cleaner(tweet_input):\n",
    "    # tweet_input = tweet_input.lower() # lowercase everything\n",
    "    tweet_input = tweet_input.encode('ascii', 'ignore').decode()  # remove unicode characters\n",
    "    tweet_input = re.sub(r'https*\\S+', ' ', tweet_input) # remove links\n",
    "    tweet_input = re.sub(r'http*\\S+', ' ', tweet_input)\n",
    "    \n",
    "    # cleaning up text\n",
    "    tweet_input = re.sub(r'(\\\\n\\\\n)',' ',tweet_input) # catches double newlines\n",
    "    tweet_input = re.sub(r'\\\\n',' ',tweet_input)\n",
    "    tweet_input = re.sub(r':\\s[https]\\S+','.',tweet_input) # remove links that are quoted at the end of tweet\n",
    "    tweet_input = re.sub(r'\\s?https\\S+','',tweet_input)\n",
    "    tweet_input = re.sub(r'\\s?http\\S+','',tweet_input)\n",
    "    tweet_input = re.sub(r'(Via)\\s@\\S+','', tweet_input)\n",
    "    tweet_input = re.sub(r'\\s(via)\\s@\\S+','', tweet_input)\n",
    "    tweet_input = re.sub(r'@','',tweet_input)\n",
    "    tweet_input = re.sub(r'&amp;','and', tweet_input)\n",
    "    tweet_input = re.sub(r'#', '',tweet_input)\n",
    "    tweet_input = re.sub(r'w/', 'with ', tweet_input)\n",
    "    tweet_input = re.sub(r'\\xa0',' ',tweet_input)\n",
    "    #tweet_input = re.sub(r'\\'','999', tweet_input) # maybe delete\n",
    "    tweet_input = re.sub(r':[!\\s]',': ',tweet_input) # maybe delete\n",
    "\n",
    "    # shortened words in \"twitter language\"\n",
    "    tweet_input = re.sub(r'DidYouKnow', 'Did you know',tweet_input)\n",
    "    tweet_input = re.sub(r'DYK', 'Did you know',tweet_input)\n",
    "    tweet_input = re.sub(r'ICYMI', 'In case you missed it',tweet_input)\n",
    "    tweet_input = re.sub(r'FYI', 'For your information',tweet_input)\n",
    "\n",
    "    # heavier cleaning\n",
    "    tweet_input = re.sub(r'oilandgas', 'oil and gas', tweet_input)\n",
    "    tweet_input = re.sub(r'natgas', 'natural gas', tweet_input)\n",
    "\n",
    "    \n",
    "    #text = re.sub(r'\\w*\\d+\\w*', '', text)\n",
    "    tweet_input = re.sub(r'\\s{2,}', ' ', tweet_input)\n",
    "    #text = re.sub(r'\\'\\w+', '', text) \n",
    "    #text = re.sub(r'\\s[^\\w\\s]\\s', '', text)\n",
    "\n",
    "    return tweet_input\n",
    "\n",
    "\n",
    "# function to remove specific hashtags within the tweet\n",
    "def hashtag_cleaner(tweet_input, hashtag):\n",
    "    tweet_input = re.sub(hashtag, '', tweet_input)\n",
    "    return tweet_input\n",
    "\n",
    "\n",
    "\n",
    "# function to transform the date format of \"created_at\" into datetime object\n",
    "def tweet_date_to_datetime(tweet_date_input):\n",
    "    datetime_object = datetime.strptime(tweet_date_input, '%Y-%m-%dT%H:%M:%S.000Z')\n",
    "    return datetime_object\n",
    "\n",
    "\n",
    "\n",
    "# special case for \"The Parliamentarian Twitter Dataset\": function to transform the date format of \"created_at\" into datetime object\n",
    "# format: \"2021-01-03 22:19:37\"\n",
    "def tweet_date_to_datetime_alternative(tweet_date_input):\n",
    "    datetime_object = datetime.strptime(tweet_date_input, '%Y-%m-%d %H:%M:%S')\n",
    "    return datetime_object\n",
    "\n",
    "\n",
    "# another special case for \"The Parliamentarian Twitter Dataset\": function to transform the date format of \"created_at\" into datetime object\n",
    "# here the days are written as abreviations of the word (e.g. Tuesday = Tue)\n",
    "# format to parse: \"Wed Nov 08 15:48:37 +0000 2017\"\n",
    "def tweet_date_to_datetime_words(tweet_date_input):\n",
    "    datetime_object = datetime.strptime(tweet_date_input, '%a %b %d %H:%M:%S +0000 %Y')\n",
    "    return datetime_object\n",
    "\n",
    "\n",
    "\n",
    "# function to count the number of words in the input cleaned text\n",
    "\n",
    "def word_count(sentence):\n",
    "    n_words = len(sentence.split())\n",
    "    return n_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the .csv\n",
    "\n",
    "# define directory and extract the .csv files\n",
    "dir_path_downplaying = \"tweets/downplaying/\"\n",
    "dir_path_underscoring = \"tweets/underscoring/\"\n",
    "dir_path_politicians = \"tweets/politicians/\"\n",
    "dir_path_greenpeace_API = \"tweets/greenpeace_API/\"\n",
    "dir_path_selected_polit = \"tweets/selected_polit/\"\n",
    "\n",
    "\n",
    "filenames_only_csv_downplaying = glob.glob(dir_path_downplaying+\"*.csv\")\n",
    "filenames_only_csv_underscoring = glob.glob(dir_path_underscoring+\"*.csv\")\n",
    "filenames_only_csv_politicians = glob.glob(dir_path_politicians+\"*.csv\")\n",
    "filenames_only_csv_greenpeace_API = glob.glob(dir_path_greenpeace_API+\"*.csv\")\n",
    "filenames_only_csv_selected_polit = glob.glob(dir_path_selected_polit+\"*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tweets/selected_polit\\\\algore_09012023.csv',\n",
       " 'tweets/selected_polit\\\\AOC_09042023.csv',\n",
       " 'tweets/selected_polit\\\\BarackObama_09012023.csv',\n",
       " 'tweets/selected_polit\\\\BetoORourke__09042023.csv',\n",
       " 'tweets/selected_polit\\\\chelliepingree_09042023.csv',\n",
       " 'tweets/selected_polit\\\\CongressmanHice_09042023.csv',\n",
       " 'tweets/selected_polit\\\\davidcicilline_09042023.csv',\n",
       " 'tweets/selected_polit\\\\EliseStefanik_09042023.csv',\n",
       " 'tweets/selected_polit\\\\JeffFortenberry_09042023.csv',\n",
       " 'tweets/selected_polit\\\\LtGovDennyHeck_09042023.csv',\n",
       " 'tweets/selected_polit\\\\Malinowski_09042023.csv',\n",
       " 'tweets/selected_polit\\\\MartinHeinrich_09042023.csv',\n",
       " 'tweets/selected_polit\\\\MikeCrapo_09042023.csv',\n",
       " 'tweets/selected_polit\\\\PeteSessions_09042023.csv',\n",
       " 'tweets/selected_polit\\\\RepAlexMooney_09042023.csv',\n",
       " 'tweets/selected_polit\\\\RepAndyBarr_09042023.csv',\n",
       " 'tweets/selected_polit\\\\RepAndyHarrisMD_09042023.csv',\n",
       " 'tweets/selected_polit\\\\RepAnnieKuster_09042023.csv',\n",
       " 'tweets/selected_polit\\\\RepBera_09042023.csv',\n",
       " 'tweets/selected_polit\\\\RepBillJohnson_09042023.csv',\n",
       " 'tweets/selected_polit\\\\RepBrendanBoyle_09042023.csv',\n",
       " 'tweets/selected_polit\\\\RepBrianFitz_09042023.csv',\n",
       " 'tweets/selected_polit\\\\RepCartwright_09042023.csv',\n",
       " 'tweets/selected_polit\\\\RepCheri_09042023.csv',\n",
       " 'tweets/selected_polit\\\\RepCloudTX_09042023.csv',\n",
       " 'tweets/selected_polit\\\\RepCuellar_09042023_full.csv',\n",
       " 'tweets/selected_polit\\\\RepDanCrenshaw_09042023_full.csv',\n",
       " 'tweets/selected_polit\\\\RepDebDingell_09042023.csv',\n",
       " 'tweets/selected_polit\\\\RepDonBeyer_09042023.csv',\n",
       " 'tweets/selected_polit\\\\RepFilemonVela_09042023.csv',\n",
       " 'tweets/selected_polit\\\\RepFletcher_09042023_full.csv',\n",
       " 'tweets/selected_polit\\\\RepGosar_09042023.csv',\n",
       " 'tweets/selected_polit\\\\RepHuizenga_09042023.csv',\n",
       " 'tweets/selected_polit\\\\RepJackBergman_09042023.csv',\n",
       " 'tweets/selected_polit\\\\RepJamesClyburn_09042023.csv',\n",
       " 'tweets/selected_polit\\\\RepJeffries_09042023.csv',\n",
       " 'tweets/selected_polit\\\\RepJimBanks_09042023.csv',\n",
       " 'tweets/selected_polit\\\\RepJudyChu_09042023.csv',\n",
       " 'tweets/selected_polit\\\\RepLizCheney_09042023_full.csv',\n",
       " 'tweets/selected_polit\\\\RepMikeQuigley_09042023_full.csv',\n",
       " 'tweets/selected_polit\\\\RepMoolenaar_09042023.csv',\n",
       " 'tweets/selected_polit\\\\RepPaulTonko__09042023_full.csv',\n",
       " 'tweets/selected_polit\\\\RepPfluger_09042023_full.csv',\n",
       " 'tweets/selected_polit\\\\RepRashida_09042023.csv',\n",
       " 'tweets/selected_polit\\\\RepRickAllen_09042023.csv',\n",
       " 'tweets/selected_polit\\\\RepRobinKelly_09042023.csv',\n",
       " 'tweets/selected_polit\\\\RepRodBlum_09042023.csv',\n",
       " 'tweets/selected_polit\\\\RepSarbanes_09042023.csv',\n",
       " 'tweets/selected_polit\\\\RepScottPeters_09042023.csv',\n",
       " 'tweets/selected_polit\\\\RepSmucker_09042023.csv',\n",
       " 'tweets/selected_polit\\\\RepSpeier_09042023.csv',\n",
       " 'tweets/selected_polit\\\\RepTedDeutch_09042023.csv',\n",
       " 'tweets/selected_polit\\\\RepTrentKelly_09042023.csv',\n",
       " 'tweets/selected_polit\\\\RepWilson_09042023.csv',\n",
       " 'tweets/selected_polit\\\\RogerMarshallMD_09042023.csv',\n",
       " 'tweets/selected_polit\\\\SecFudge_09042023.csv',\n",
       " 'tweets/selected_polit\\\\SenatorLankford_09042023_full.csv',\n",
       " 'tweets/selected_polit\\\\SenatorSinema_09042023.csv',\n",
       " 'tweets/selected_polit\\\\SenJohnHoeven_09042023.csv',\n",
       " 'tweets/selected_polit\\\\SenJoniErnst_09042023.csv',\n",
       " 'tweets/selected_polit\\\\SenKevinCramer_09042023.csv',\n",
       " 'tweets/selected_polit\\\\SenSanders_09042023.csv',\n",
       " 'tweets/selected_polit\\\\Sen_JoeManchin_09042023_full.csv',\n",
       " 'tweets/selected_polit\\\\sethmoulton_09042023.csv',\n",
       " 'tweets/selected_polit\\\\SpeakerMcCarthy_09042023_full.csv',\n",
       " 'tweets/selected_polit\\\\StaceyPlaskett_09042023.csv',\n",
       " 'tweets/selected_polit\\\\SteveScalise__09042023_full.csv',\n",
       " 'tweets/selected_polit\\\\tedcruz_09042023.csv',\n",
       " 'tweets/selected_polit\\\\USRepKeating_09042023.csv',\n",
       " 'tweets/selected_polit\\\\USRepMikeDoyle_09042023.csv',\n",
       " 'tweets/selected_polit\\\\WesleyHuntTX_09042023_full.csv',\n",
       " 'tweets/selected_polit\\\\WestermanAR_09042023.csv',\n",
       " 'tweets/selected_polit\\\\WhipKClark_09042023.csv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames_only_csv_selected_polit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tweet_database = pd.read_csv(filenames_only_csv[0], encoding='UTF-8', low_memory=False)\n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# column_selection_tweets = [ 'id',\n",
    "#                             'text',\n",
    "#                             'author_id',\n",
    "#                             #'author_username',\n",
    "#                             'author.name',\n",
    "#                             'created_at',\n",
    "#                             'entities.hashtags',\n",
    "#                             'entities.mentions',\n",
    "#                             'entities.urls',\n",
    "#                             'lang',\n",
    "#                             'in_reply_to_user_id',\n",
    "#                             '__twarc.retrieved_at']\n",
    "# \n",
    "# tweet_database_col_selec = tweet_database[column_selection_tweets]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combining multiple tweets to databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the current .csv file as a dataframe\n",
    "\n",
    "# twarc data dictionary: \n",
    "# https://developer.twitter.com/en/docs/twitter-api/data-dictionary/introduction\n",
    "# https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/tweet\n",
    "\n",
    "### what columns to keep:\n",
    "\n",
    "# id:                   unique identifier of the requested tweet\n",
    "# text:                 actual UTF-8 text of the Tweet\n",
    "# author_id:            unique identifier of the User who posted tweet\n",
    "# author.name:          Name of the User\n",
    "# created_at:           creation time (for timeseries analysis)\n",
    "# entities.hashtags:    could be helpful for cleaning the tweets\n",
    "# entities.mentions:    could be helpful for cleaning the tweets\n",
    "# entities.urls:        coule be helpful for cleaning the tweets\n",
    "# lang:                 filter to english text\n",
    "# in_reply_to_user_id:  use this to detect threads\n",
    "# __twarc.retrieved_at: date when the tweet was scraped through twarc \n",
    "\n",
    "\n",
    "# filter from the 83 columns\n",
    "column_selection_tweets = [ 'id',\n",
    "                            'text',\n",
    "                            'author_id',\n",
    "                            #'author_username',\n",
    "                            'author.name',\n",
    "                            'created_at',\n",
    "                            'entities.hashtags',\n",
    "                            'entities.mentions',\n",
    "                            'entities.urls',\n",
    "                            'lang',\n",
    "                            'in_reply_to_user_id',\n",
    "                            '__twarc.retrieved_at',\n",
    "                            'author.public_metrics.followers_count',\n",
    "                            ]\n",
    "\n",
    "# rearranged for clarity\n",
    "column_rearranged =       [ 'id',\n",
    "                            'text',\n",
    "                            'cleaned_text',\n",
    "                            'climate_related',\n",
    "                            'downplaying',\n",
    "                            'type',\n",
    "                            'author_id',\n",
    "                            'author.name',\n",
    "                            'created_at',\n",
    "                            'date',\n",
    "                            'year',\n",
    "                            'entities.hashtags',\n",
    "                            'entities.mentions',\n",
    "                            'entities.urls',\n",
    "                            'lang',\n",
    "                            'in_reply_to_user_id',\n",
    "                            '__twarc.retrieved_at',\n",
    "                            'author.public_metrics.followers_count',\n",
    "                            ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# pre-allocation of combined dataframes\n",
    "combined_df_downplaying = pd.DataFrame(columns=column_selection_tweets)\n",
    "combined_df_underscoring = pd.DataFrame(columns=column_selection_tweets)\n",
    "combined_df_politicians = pd.DataFrame(columns=column_selection_tweets)\n",
    "combined_df_greenpeace_API = pd.DataFrame(columns=column_selection_tweets)\n",
    "combined_df_selected_polit = pd.DataFrame(columns=column_selection_tweets)\n",
    "\n",
    "\n",
    "# combining the individual tweets\n",
    "\n",
    "# downplaying\n",
    "for name in range(len(filenames_only_csv_downplaying)):\n",
    "    current_tweet_database = pd.read_csv(filenames_only_csv_downplaying[name], encoding='UTF-8', low_memory=False)\n",
    "    current_tweet_database_col_selec = current_tweet_database[column_selection_tweets]\n",
    "\n",
    "    combined_df_downplaying = pd.concat([combined_df_downplaying,\n",
    "                            current_tweet_database_col_selec])\n",
    "\n",
    "    combined_df_downplaying.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# underscoring\n",
    "for name in range(len(filenames_only_csv_underscoring)):\n",
    "    current_tweet_database = pd.read_csv(filenames_only_csv_underscoring[name], encoding='UTF-8', low_memory=False)\n",
    "    current_tweet_database_col_selec = current_tweet_database[column_selection_tweets]\n",
    "\n",
    "    combined_df_underscoring = pd.concat([combined_df_underscoring,\n",
    "                            current_tweet_database_col_selec])\n",
    "\n",
    "    combined_df_underscoring.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# politicians\n",
    "for name in range(len(filenames_only_csv_politicians)):\n",
    "    current_tweet_database = pd.read_csv(filenames_only_csv_politicians[name], encoding='UTF-8', low_memory=False)\n",
    "    current_tweet_database_col_selec = current_tweet_database[column_selection_tweets]\n",
    "\n",
    "    combined_df_politicians = pd.concat([combined_df_politicians,\n",
    "                            current_tweet_database_col_selec])\n",
    "\n",
    "    combined_df_politicians.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Greenpeace & API\n",
    "for name in range(len(filenames_only_csv_greenpeace_API)):\n",
    "    current_tweet_database = pd.read_csv(filenames_only_csv_greenpeace_API[name], encoding='UTF-8', low_memory=False)\n",
    "    current_tweet_database_col_selec = current_tweet_database[column_selection_tweets]\n",
    "\n",
    "    combined_df_greenpeace_API = pd.concat([combined_df_greenpeace_API,\n",
    "                            current_tweet_database_col_selec])\n",
    "\n",
    "    combined_df_greenpeace_API.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# selected politicians\n",
    "for name in range(len(filenames_only_csv_selected_polit)):\n",
    "    current_tweet_database = pd.read_csv(filenames_only_csv_selected_polit[name], encoding='UTF-8', low_memory=False)\n",
    "    current_tweet_database_col_selec = current_tweet_database[column_selection_tweets]\n",
    "\n",
    "    combined_df_selected_polit = pd.concat([combined_df_selected_polit,\n",
    "                            current_tweet_database_col_selec])\n",
    "\n",
    "    combined_df_selected_polit.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "combined_df_downplaying['cleaned_text'] = combined_df_downplaying['text'].apply(tweet_cleaner) # cleaning the tweet\n",
    "combined_df_downplaying['date'] = combined_df_downplaying['created_at'].apply(tweet_date_to_datetime) # transforming twitter date format to .date format\n",
    "combined_df_downplaying['year'] = combined_df_downplaying['date'].dt.year # extracting the year for timeseries analysis\n",
    "combined_df_downplaying['climate_related'] = 1 # pre-labelled climate related (will be checked with climateBERT)\n",
    "combined_df_downplaying['downplaying'] = 1 # pre-labelled downplaying based on source of tweets\n",
    "combined_df_downplaying['type'] = 'tweet' # type of the text\n",
    "combined_df_downplaying = combined_df_downplaying.loc[combined_df_downplaying['lang'] == 'en'] # filtering for English tweets\n",
    "\n",
    " \n",
    "combined_df_underscoring['cleaned_text'] = combined_df_underscoring['text'].apply(tweet_cleaner)\n",
    "combined_df_underscoring['date'] = combined_df_underscoring['created_at'].apply(tweet_date_to_datetime)\n",
    "combined_df_underscoring['year'] = combined_df_underscoring['date'].dt.year\n",
    "combined_df_underscoring['climate_related'] = 1\n",
    "combined_df_underscoring['downplaying'] = 0\n",
    "combined_df_underscoring['type'] = 'tweet'\n",
    "combined_df_underscoring = combined_df_underscoring.loc[combined_df_underscoring['lang'] == 'en']\n",
    "\n",
    "\n",
    " \n",
    "combined_df_politicians['cleaned_text'] = combined_df_politicians['text'].apply(tweet_cleaner)\n",
    "combined_df_politicians['date'] = combined_df_politicians['created_at'].apply(tweet_date_to_datetime)\n",
    "combined_df_politicians['year'] = combined_df_politicians['date'].dt.year\n",
    "combined_df_politicians['climate_related'] = 1\n",
    "combined_df_politicians['downplaying'] = ''\n",
    "combined_df_politicians['type'] = 'tweet'\n",
    "combined_df_politicians = combined_df_politicians.loc[combined_df_politicians['lang'] == 'en']\n",
    "\n",
    "\n",
    "\n",
    "combined_df_greenpeace_API['cleaned_text'] = combined_df_greenpeace_API['text'].apply(tweet_cleaner)\n",
    "combined_df_greenpeace_API['date'] = combined_df_greenpeace_API['created_at'].apply(tweet_date_to_datetime)\n",
    "combined_df_greenpeace_API['year'] = combined_df_greenpeace_API['date'].dt.year\n",
    "combined_df_greenpeace_API['climate_related'] = 1\n",
    "combined_df_greenpeace_API['downplaying'] = ''\n",
    "combined_df_greenpeace_API['type'] = 'tweet'\n",
    "combined_df_greenpeace_API = combined_df_greenpeace_API.loc[combined_df_greenpeace_API['lang'] == 'en']\n",
    "\n",
    "\n",
    "\n",
    "combined_df_selected_polit['cleaned_text'] = combined_df_selected_polit['text'].apply(tweet_cleaner)\n",
    "combined_df_selected_polit['date'] = combined_df_selected_polit['created_at'].apply(tweet_date_to_datetime)\n",
    "combined_df_selected_polit['year'] = combined_df_selected_polit['date'].dt.year\n",
    "combined_df_selected_polit['climate_related'] = 1\n",
    "combined_df_selected_polit['downplaying'] = ''\n",
    "combined_df_selected_polit['type'] = 'tweet'\n",
    "combined_df_selected_polit = combined_df_selected_polit.loc[combined_df_selected_polit['lang'] == 'en']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "combined_df_downplaying = combined_df_downplaying[column_rearranged]\n",
    "combined_df_underscoring = combined_df_underscoring[column_rearranged]\n",
    "combined_df_politicians = combined_df_politicians[column_rearranged]\n",
    "combined_df_greenpeace_API = combined_df_greenpeace_API[column_rearranged]\n",
    "combined_df_selected_polit = combined_df_selected_polit[column_rearranged]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "special case for Donald Trump (source: https://www.thetrumparchive.com/faq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_donald_trump = pd.read_csv(\"tweets/politicians/trump/trump_tweets_01-08-2021.csv\",usecols=['id','text','isRetweet','date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_donald_trump['cleaned_text'] = tweets_donald_trump['text'].apply(tweet_cleaner)\n",
    "tweets_donald_trump['date'] = tweets_donald_trump['date'].apply(tweet_date_to_datetime_alternative)\n",
    "tweets_donald_trump['year'] = tweets_donald_trump['date'].dt.year\n",
    "tweets_donald_trump['climate_related'] = 1\n",
    "tweets_donald_trump['downplaying'] = ' '\n",
    "tweets_donald_trump['type'] = 'tweet'\n",
    "tweets_donald_trump['author.name'] = 'Donald Trump'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_donald_trump['word_count'] = tweets_donald_trump['cleaned_text'].apply(word_count).astype('int')\n",
    "tweets_donald_trump_filtered = tweets_donald_trump[(tweets_donald_trump['word_count'] >= 6) & (tweets_donald_trump['isRetweet'] == 'f')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_politicians_with_trump = pd.concat([combined_df_politicians,tweets_donald_trump_filtered])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_selected_politicians = pd.concat([combined_df_selected_polit,tweets_donald_trump_filtered]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_greenpeace_API.reset_index(drop = True, inplace = True)\n",
    "combined_df_greenpeace_API['downplaying'] = [1 if name == 'American Petroleum Institute' else 0 for name in combined_df_greenpeace_API['author.name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the combined files as csv\n",
    "\n",
    "combined_df_downplaying.to_csv('downplaying_tweets_out.csv')\n",
    "combined_df_underscoring.to_csv('underscoring_tweets_out.csv')\n",
    "combined_df_politicians.to_csv('politicians_tweets_out.csv')\n",
    "combined_df_greenpeace_API.to_csv('greenpeace_API_tweets_out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_selected_politicians.to_parquet('selected_politicians_tweets.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_politicians_with_trump.to_csv('politicians_tweets_with_trump_out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_climate_influencers = combined_df_politicians[combined_df_politicians['author.name'].isin(['George Monbiot', 'Steve Milloy', 'Tony Heller', 'Bjorn Lomborg', 'Mike Hudema'])].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_climate_influencers.to_csv('climate_influencers_tweets_out.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Politician Dataset \n",
    "(rehydrated from https://figshare.com/articles/dataset/The_Twitter_Parliamentarian_Database/10120685?file=18238628)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "member info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "member_info = pd.read_csv(\"tweets/politicians/rehydrated/2020_member_info.csv\", encoding = 'utf16', engine = 'python')\n",
    "\n",
    "# member info includes additional interesting information for analysis later on (e.g. function)\n",
    "member_info_col_selected = member_info[['country',\n",
    "                                        #'region',\n",
    "                                        'name',\n",
    "                                        'party', \n",
    "                                        'uid',\n",
    "                                        ]].copy()\n",
    "\n",
    "# only look at US politicians\n",
    "member_info_only_us = member_info_col_selected[member_info_col_selected['country']=='United States']\n",
    "\n",
    "# filter duplicates\n",
    "member_info_only_us_no_duplicates = member_info_only_us.drop_duplicates(subset=['uid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only look at UK politicians\n",
    "member_info_only_uk = member_info_col_selected[member_info_col_selected['country']=='United Kingdom']\n",
    "\n",
    "# filter duplicates\n",
    "member_info_only_uk_no_duplicates = member_info_only_uk.drop_duplicates(subset=['uid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only look at Australian politicians\n",
    "member_info_only_australia = member_info_col_selected[member_info_col_selected['country']=='Australia']\n",
    "\n",
    "# filter duplicates\n",
    "member_info_only_australia_no_duplicates = member_info_only_australia.drop_duplicates(subset=['uid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#member_info_only_us_no_duplicates[['name','region']].to_csv(\"region_mapping.csv\", sep = \";\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2021"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucas\\AppData\\Local\\Temp\\ipykernel_15652\\2114821690.py:3: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data_politicians_2021 = pd.read_csv('tweets/politicians/rehydrated/2021.csv')\n"
     ]
    }
   ],
   "source": [
    "# filtering tweet ids from the United States to not overdraft the Twitter API requests\n",
    "\n",
    "data_politicians_2021 = pd.read_csv('tweets/politicians/rehydrated/2021.csv')\n",
    "\n",
    "data_politicians_2021.columns = ['country', 'party', 'name', 'uid', 'district','created_at', 'id']\n",
    "\n",
    "data_politicians_2021_only_us = data_politicians_2021[data_politicians_2021['country']=='United States']\n",
    "\n",
    "data_politicians_2021_only_us_no_duplicates = data_politicians_2021_only_us.drop_duplicates(subset=['id'])\n",
    "\n",
    "\n",
    "\n",
    "# exporting the tweet ids to a txt file as input for hydrator\n",
    "\n",
    "#  tweet_ids_politicians_2021_only_us_no_duplicates = data_politicians_2021_only_us_no_duplicates['id']\n",
    "#  \n",
    "#  with open('tweets/politicians/rehydrated/tweet_ids_politicians_2021_only_us.txt', 'w') as f:\n",
    "#      for id in tweet_ids_politicians_2021_only_us_no_duplicates:\n",
    "#          f.write(str(id))\n",
    "#          f.write('\\n')\n",
    "\n",
    "\n",
    "# reloading the rehydrated 2021 data:\n",
    "\n",
    "rehydrated_tweets_politicians_2021 = pd.read_csv('tweets/politicians/rehydrated/parliamentarian_dataset_2021_only_us.csv')\n",
    "rehydrated_tweets_politicians_2021_col_selected = rehydrated_tweets_politicians_2021[['text','id', 'retweet_screen_name','lang','user_followers_count']].copy()\n",
    "\n",
    "joined_tweet_and_info_2021 = pd.merge(rehydrated_tweets_politicians_2021_col_selected, data_politicians_2021_only_us_no_duplicates, how = 'inner', on = 'id')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "same for UK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucas\\AppData\\Local\\Temp\\ipykernel_2096\\2213642749.py:19: DtypeWarning: Columns (0,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  rehydrated_tweets_politicians_2021_uk = pd.read_csv('tweets/politicians/rehydrated/parliamentarian_dataset_2021_only_uk.csv')\n"
     ]
    }
   ],
   "source": [
    "data_politicians_2021_only_uk = data_politicians_2021[data_politicians_2021['country']=='United Kingdom']\n",
    "\n",
    "data_politicians_2021_only_uk_no_duplicates = data_politicians_2021_only_uk.drop_duplicates(subset=['id'])\n",
    "\n",
    "\n",
    "\n",
    "# exporting the tweet ids to a txt file as input for hydrator\n",
    "\n",
    "# tweet_ids_politicians_2021_only_uk_no_duplicates = data_politicians_2021_only_uk_no_duplicates['id']\n",
    "# \n",
    "# with open('tweets/politicians/rehydrated/tweet_ids_politicians_2021_only_uk.txt', 'w') as f:\n",
    "#     for id in tweet_ids_politicians_2021_only_uk_no_duplicates:\n",
    "#         f.write(str(id))\n",
    "#         f.write('\\n')\n",
    "\n",
    "\n",
    "# reloading the rehydrated 2021 data:\n",
    "\n",
    "rehydrated_tweets_politicians_2021_uk = pd.read_csv('tweets/politicians/rehydrated/parliamentarian_dataset_2021_only_uk.csv')\n",
    "rehydrated_tweets_politicians_2021_uk_col_selected = rehydrated_tweets_politicians_2021_uk[['text','id', 'retweet_screen_name','lang','user_followers_count']].copy()\n",
    "\n",
    "joined_tweet_and_info_2021_uk = pd.merge(rehydrated_tweets_politicians_2021_uk_col_selected, data_politicians_2021_only_uk_no_duplicates, how = 'inner', on = 'id')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "same for Australia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucas\\AppData\\Local\\Temp\\ipykernel_2096\\923256106.py:18: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  rehydrated_tweets_politicians_2021_australia = pd.read_csv('tweets/politicians/rehydrated/parliamentarian_dataset_2021_only_australia.csv')\n"
     ]
    }
   ],
   "source": [
    "data_politicians_2021_only_australia = data_politicians_2021[data_politicians_2021['country']=='Australia']\n",
    "\n",
    "data_politicians_2021_only_australia_no_duplicates = data_politicians_2021_only_australia.drop_duplicates(subset=['id'])\n",
    "\n",
    "\n",
    "\n",
    "# exporting the tweet ids to a txt file as input for hydrator\n",
    "\n",
    "# tweet_ids_politicians_2021_only_australia_no_duplicates = data_politicians_2021_only_australia_no_duplicates['id']\n",
    "# \n",
    "# with open('tweets/politicians/rehydrated/tweet_ids_politicians_2021_only_australia.txt', 'w') as f:\n",
    "#     for id in tweet_ids_politicians_2021_only_australia_no_duplicates:\n",
    "#         f.write(str(id))\n",
    "#         f.write('\\n')\n",
    "\n",
    "# reloading the rehydrated 2021 data:\n",
    "\n",
    "rehydrated_tweets_politicians_2021_australia = pd.read_csv('tweets/politicians/rehydrated/parliamentarian_dataset_2021_only_australia.csv')\n",
    "rehydrated_tweets_politicians_2021_australia_col_selected = rehydrated_tweets_politicians_2021_australia[['text','id', 'retweet_screen_name','lang', 'user_followers_count']].copy()\n",
    "\n",
    "joined_tweet_and_info_2021_australia = pd.merge(rehydrated_tweets_politicians_2021_australia_col_selected, data_politicians_2021_only_australia_no_duplicates, how = 'inner', on = 'id')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2020 and older (from all_twitter_ids file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating batches of the original dataset to rehydrate (batches of 1.5 million tweets)\n",
    "\n",
    "# tweet_ids_politicians_2020_second_batch = pd.read_csv(\"C:/Users/lucas/Downloads/all_tweet_ids.csv\", skiprows= 500000, index_col=0, chunksize= 1500000).get_chunk(1500000)\n",
    "# tweet_ids_politicians_2020_third_batch = pd.read_csv(\"C:/Users/lucas/Downloads/all_tweet_ids.csv\", skiprows= 2000000, index_col=0, chunksize= 1500000).get_chunk(1500000)\n",
    "# tweet_ids_politicians_2020_fourth_batch = pd.read_csv(\"C:/Users/lucas/Downloads/all_tweet_ids.csv\", skiprows= 5500000, index_col=0, chunksize= 1500000).get_chunk(1500000)\n",
    "# tweet_ids_politicians_2020_fifth_batch = pd.read_csv(\"C:/Users/lucas/Downloads/all_tweet_ids.csv\", skiprows= 8000000, index_col=0, chunksize= 1000000).get_chunk(1000000)\n",
    "# tweet_ids_politicians_2020_sixth_batch = pd.read_csv(\"C:/Users/lucas/Downloads/all_tweet_ids.csv\", skiprows= 10000000, index_col=0, chunksize= 1000000).get_chunk(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the tweet id batches to .csv files to feed into hydrator\n",
    "\n",
    "# tweet_ids_politicians_2020_second_batch.to_csv('tweets/politicians/rehydrated/tweet_ids_politicians_2020_second_batch.csv')\n",
    "# tweet_ids_politicians_2020_third_batch.to_csv('tweets/politicians/rehydrated/tweet_ids_politicians_2020_third_batch.csv')\n",
    "# tweet_ids_politicians_2020_fourth_batch.to_csv('tweets/politicians/rehydrated/tweet_ids_politicians_2020_fourth_batch.csv')\n",
    "# tweet_ids_politicians_2020_fifth_batch.to_csv('tweets/politicians/rehydrated/tweet_ids_politicians_2020_fifth_batch.csv')\n",
    "# tweet_ids_politicians_2020_sixth_batch.to_csv('tweets/politicians/rehydrated/tweet_ids_politicians_2020_sixth_batch_test.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reloading the hydrated 2020 and older data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only rehydrated around 350'000 tweet ids from the total 11'000'000 due to Twitter API request limit in first batch:\n",
    "\n",
    "rehydrated_tweets_politicians_2020_and_older_first_batch = pd.read_csv('tweets/politicians/rehydrated/parliamentarian_dataset_2020_first_batch.csv', usecols = ['text','id','user_id','created_at', 'retweet_screen_name','lang','user_followers_count'])\n",
    "\n",
    "rehydrated_tweets_politicians_2020_and_older_second_batch = pd.read_csv('tweets/politicians/rehydrated/parliamentarian_dataset_2020_second_batch.csv', usecols = ['text','id','user_id','created_at', 'retweet_screen_name','lang','user_followers_count'])\n",
    "\n",
    "rehydrated_tweets_politicians_2020_and_older_third_batch = pd.read_csv('tweets/politicians/rehydrated/parliamentarian_dataset_2020_third_batch.csv', usecols = ['text','id','user_id','created_at', 'retweet_screen_name','lang','user_followers_count'])\n",
    "\n",
    "rehydrated_tweets_politicians_2020_and_older_fourth_batch = pd.read_csv('tweets/politicians/rehydrated/parliamentarian_dataset_2020_fourth_batch.csv', usecols = ['text','id','user_id','created_at', 'retweet_screen_name','lang','user_followers_count'])\n",
    "\n",
    "rehydrated_tweets_politicians_2020_and_older_fifth_batch = pd.read_csv('tweets/politicians/rehydrated/parliamentarian_dataset_2020_fifth_batch.csv', usecols = ['text','id','user_id','created_at', 'retweet_screen_name','lang','user_followers_count'])\n",
    "\n",
    "rehydrated_tweets_politicians_2020_and_older_sixth_batch = pd.read_csv('tweets/politicians/rehydrated/parliamentarian_dataset_2020_sixth_batch.csv', usecols = ['text','id','user_id','created_at', 'retweet_screen_name','lang','user_followers_count'])\n",
    "\n",
    "\n",
    "# recombining to one large dataset\n",
    "rehydrated_combined_tweets_politicians_2020 = pd.concat([   rehydrated_tweets_politicians_2020_and_older_first_batch,\n",
    "                                                            rehydrated_tweets_politicians_2020_and_older_second_batch,\n",
    "                                                            rehydrated_tweets_politicians_2020_and_older_third_batch,\n",
    "                                                            rehydrated_tweets_politicians_2020_and_older_fourth_batch,\n",
    "                                                            rehydrated_tweets_politicians_2020_and_older_fifth_batch,\n",
    "                                                            rehydrated_tweets_politicians_2020_and_older_sixth_batch])\n",
    "\n",
    "# renaming the user_id column for coherence across other datasets\n",
    "rehydrated_combined_tweets_politicians_2020.rename(columns={'user_id' : 'uid'}, inplace = True)\n",
    "\n",
    "# filtering for english tweets\n",
    "rehydrated_combined_tweets_politicians_2020_only_english = rehydrated_combined_tweets_politicians_2020.loc[rehydrated_combined_tweets_politicians_2020['lang'] == 'en']\n",
    "\n",
    "\n",
    "joined_tweet_and_info_2020_and_older = pd.merge(rehydrated_combined_tweets_politicians_2020_only_english, member_info_only_us_no_duplicates, how = 'inner', on = 'uid')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_tweet_and_info_2020_and_older_uk = pd.merge(rehydrated_combined_tweets_politicians_2020_only_english, member_info_only_uk_no_duplicates, how = 'inner', on = 'uid')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Australia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_tweet_and_info_2020_and_older_australia = pd.merge(rehydrated_combined_tweets_politicians_2020_only_english, member_info_only_australia_no_duplicates, how = 'inner', on = 'uid')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "applying data cleaning functions & combining to joined dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_tweet_and_info_2021['cleaned_text'] = joined_tweet_and_info_2021['text'].apply(tweet_cleaner) # cleaning the tweet\n",
    "joined_tweet_and_info_2021['date'] = joined_tweet_and_info_2021['created_at'].apply(tweet_date_to_datetime_alternative) # transforming twitter date format to .date format\n",
    "joined_tweet_and_info_2021['year'] = joined_tweet_and_info_2021['date'].dt.year # extracting the year for timeseries analysis\n",
    "\n",
    "\n",
    "joined_tweet_and_info_2020_and_older['cleaned_text'] = joined_tweet_and_info_2020_and_older['text'].apply(tweet_cleaner)\n",
    "joined_tweet_and_info_2020_and_older['date'] = joined_tweet_and_info_2020_and_older['created_at'].apply(tweet_date_to_datetime_words)\n",
    "joined_tweet_and_info_2020_and_older['year'] = joined_tweet_and_info_2020_and_older['date'].dt.year \n",
    "\n",
    "\n",
    "# creating one large dataset from 2020 and 2021:\n",
    "\n",
    "joined_tweet_and_info_2020_and_older_col_selcted = joined_tweet_and_info_2020_and_older[['text','cleaned_text','name','party','country','date','year','id','uid','lang','retweet_screen_name','user_followers_count']].copy()\n",
    "\n",
    "joined_tweet_and_info_2021_col_selcted = joined_tweet_and_info_2021[['text','cleaned_text','name','party','country','date','year','id','uid','lang','retweet_screen_name','user_followers_count']].copy()\n",
    "\n",
    "full_tweet_dataset_politicians = pd.concat([joined_tweet_and_info_2020_and_older_col_selcted,\n",
    "                                            joined_tweet_and_info_2021_col_selcted],\n",
    "                                            ignore_index= True)\n",
    "\n",
    "# adding climate_related column that will later be overwritten by classifier predictions\n",
    "full_tweet_dataset_politicians['climate_related'] = 1\n",
    "\n",
    "\n",
    "# only select tweets longer than 3 words:\n",
    "full_tweet_dataset_politicians['cleaned_text_word_count'] = full_tweet_dataset_politicians['cleaned_text'].apply(word_count).astype('int')\n",
    "\n",
    "full_tweet_dataset_politicians = full_tweet_dataset_politicians[full_tweet_dataset_politicians['cleaned_text_word_count'] >= 4]\n",
    "full_tweet_dataset_politicians.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>name</th>\n",
       "      <th>party</th>\n",
       "      <th>country</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>id</th>\n",
       "      <th>uid</th>\n",
       "      <th>lang</th>\n",
       "      <th>retweet_screen_name</th>\n",
       "      <th>user_followers_count</th>\n",
       "      <th>climate_related</th>\n",
       "      <th>cleaned_text_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#FosterYouthVoices must be included in the chi...</td>\n",
       "      <td>FosterYouthVoices must be included in the chil...</td>\n",
       "      <td>bobby l. rush</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>United States</td>\n",
       "      <td>2017-05-22 19:07:07</td>\n",
       "      <td>2017</td>\n",
       "      <td>866732173252059136</td>\n",
       "      <td>305216911</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41606</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>During Natl #DrugCourtMonth, Id like to reco...</td>\n",
       "      <td>During Natl DrugCourtMonth, Id like to recogni...</td>\n",
       "      <td>bobby l. rush</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>United States</td>\n",
       "      <td>2017-05-22 21:17:22</td>\n",
       "      <td>2017</td>\n",
       "      <td>866764952052355072</td>\n",
       "      <td>305216911</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41606</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The #TrumpCuts budget is an assault on working...</td>\n",
       "      <td>The TrumpCuts budget is an assault on working ...</td>\n",
       "      <td>bobby l. rush</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>United States</td>\n",
       "      <td>2017-05-23 16:02:57</td>\n",
       "      <td>2017</td>\n",
       "      <td>867048214029099009</td>\n",
       "      <td>305216911</td>\n",
       "      <td>en</td>\n",
       "      <td>JohnYarmuth</td>\n",
       "      <td>41606</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Between #TrumpCare and #TrumpCuts, @realDonald...</td>\n",
       "      <td>Between TrumpCare and TrumpCuts, realDonaldTru...</td>\n",
       "      <td>bobby l. rush</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>United States</td>\n",
       "      <td>2017-05-24 13:38:41</td>\n",
       "      <td>2017</td>\n",
       "      <td>867374292694175746</td>\n",
       "      <td>305216911</td>\n",
       "      <td>en</td>\n",
       "      <td>RepCheri</td>\n",
       "      <td>41606</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thank you @RepBobbyRush for cosponsoring the A...</td>\n",
       "      <td>Thank you RepBobbyRush for cosponsoring the AD...</td>\n",
       "      <td>bobby l. rush</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>United States</td>\n",
       "      <td>2017-05-24 16:17:06</td>\n",
       "      <td>2017</td>\n",
       "      <td>867414162800095232</td>\n",
       "      <td>305216911</td>\n",
       "      <td>en</td>\n",
       "      <td>AAHOA</td>\n",
       "      <td>41606</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  #FosterYouthVoices must be included in the chi...   \n",
       "1  During Natl #DrugCourtMonth, Id like to reco...   \n",
       "2  The #TrumpCuts budget is an assault on working...   \n",
       "3  Between #TrumpCare and #TrumpCuts, @realDonald...   \n",
       "4  Thank you @RepBobbyRush for cosponsoring the A...   \n",
       "\n",
       "                                        cleaned_text           name     party  \\\n",
       "0  FosterYouthVoices must be included in the chil...  bobby l. rush  Democrat   \n",
       "1  During Natl DrugCourtMonth, Id like to recogni...  bobby l. rush  Democrat   \n",
       "2  The TrumpCuts budget is an assault on working ...  bobby l. rush  Democrat   \n",
       "3  Between TrumpCare and TrumpCuts, realDonaldTru...  bobby l. rush  Democrat   \n",
       "4  Thank you RepBobbyRush for cosponsoring the AD...  bobby l. rush  Democrat   \n",
       "\n",
       "         country                date  year                  id        uid  \\\n",
       "0  United States 2017-05-22 19:07:07  2017  866732173252059136  305216911   \n",
       "1  United States 2017-05-22 21:17:22  2017  866764952052355072  305216911   \n",
       "2  United States 2017-05-23 16:02:57  2017  867048214029099009  305216911   \n",
       "3  United States 2017-05-24 13:38:41  2017  867374292694175746  305216911   \n",
       "4  United States 2017-05-24 16:17:06  2017  867414162800095232  305216911   \n",
       "\n",
       "  lang retweet_screen_name  user_followers_count  climate_related  \\\n",
       "0   en                 NaN                 41606                1   \n",
       "1   en                 NaN                 41606                1   \n",
       "2   en         JohnYarmuth                 41606                1   \n",
       "3   en            RepCheri                 41606                1   \n",
       "4   en               AAHOA                 41606                1   \n",
       "\n",
       "   cleaned_text_word_count  \n",
       "0                       19  \n",
       "1                       22  \n",
       "2                       22  \n",
       "3                       21  \n",
       "4                       20  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_tweet_dataset_politicians.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21098"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for duplicated tweets (probably due to retweets)\n",
    "\n",
    "sum(full_tweet_dataset_politicians['cleaned_text'].duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "415"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total politicians in the sampled dataset\n",
    "len(full_tweet_dataset_politicians['name'].unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_tweet_and_info_2021_uk['cleaned_text'] = joined_tweet_and_info_2021_uk['text'].apply(tweet_cleaner) # cleaning the tweet\n",
    "joined_tweet_and_info_2021_uk['date'] = joined_tweet_and_info_2021_uk['created_at'].apply(tweet_date_to_datetime_alternative) # transforming twitter date format to .date format\n",
    "joined_tweet_and_info_2021_uk['year'] = joined_tweet_and_info_2021_uk['date'].dt.year # extracting the year for timeseries analysis\n",
    "\n",
    "\n",
    "joined_tweet_and_info_2020_and_older_uk['cleaned_text'] = joined_tweet_and_info_2020_and_older_uk['text'].apply(tweet_cleaner)\n",
    "joined_tweet_and_info_2020_and_older_uk['date'] = joined_tweet_and_info_2020_and_older_uk['created_at'].apply(tweet_date_to_datetime_words)\n",
    "joined_tweet_and_info_2020_and_older_uk['year'] = joined_tweet_and_info_2020_and_older_uk['date'].dt.year \n",
    "\n",
    "\n",
    "# creating one large dataset from 2020 and 2021:\n",
    "\n",
    "joined_tweet_and_info_2020_and_older_uk_col_selcted = joined_tweet_and_info_2020_and_older_uk[['text','cleaned_text','name','party','country','date','year','id','uid','lang','retweet_screen_name','user_followers_count']].copy()\n",
    "\n",
    "joined_tweet_and_info_2021_uk_col_selcted = joined_tweet_and_info_2021_uk[['text','cleaned_text','name','party','country','date','year','id','uid','lang','retweet_screen_name','user_followers_count']].copy()\n",
    "\n",
    "full_tweet_dataset_politicians_uk = pd.concat([joined_tweet_and_info_2020_and_older_uk_col_selcted,\n",
    "                                            joined_tweet_and_info_2021_uk_col_selcted],\n",
    "                                            ignore_index= True)\n",
    "\n",
    "# adding climate_related column that will later be overwritten by classifier predictions\n",
    "full_tweet_dataset_politicians_uk['climate_related'] = 1\n",
    "\n",
    "\n",
    "# only select tweets longer than 3 words:\n",
    "full_tweet_dataset_politicians_uk['cleaned_text_word_count'] = full_tweet_dataset_politicians_uk['cleaned_text'].apply(word_count).astype('int')\n",
    "\n",
    "full_tweet_dataset_politicians_uk = full_tweet_dataset_politicians_uk[full_tweet_dataset_politicians_uk['cleaned_text_word_count'] >= 4]\n",
    "full_tweet_dataset_politicians_uk.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Labour', 'Conservative', 'Labour Co-op',\n",
       "       'Scottish National Party', 'Liberal Democrat',\n",
       "       'Democratic Unionist Party', 'Plaid Cymru', 'Sinn Fin',\n",
       "       'Independent', 'Green Party', 'Alliance',\n",
       "       'Social Democratic & Labour Party'], dtype=object)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_tweet_dataset_politicians_uk['party'].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Australia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_tweet_and_info_2021_australia['cleaned_text'] = joined_tweet_and_info_2021_australia['text'].apply(tweet_cleaner) # cleaning the tweet\n",
    "joined_tweet_and_info_2021_australia['date'] = joined_tweet_and_info_2021_australia['created_at'].apply(tweet_date_to_datetime_alternative) # transforming twitter date format to .date format\n",
    "joined_tweet_and_info_2021_australia['year'] = joined_tweet_and_info_2021_australia['date'].dt.year # extracting the year for timeseries analysis\n",
    "\n",
    "\n",
    "joined_tweet_and_info_2020_and_older_australia['cleaned_text'] = joined_tweet_and_info_2020_and_older_australia['text'].apply(tweet_cleaner)\n",
    "joined_tweet_and_info_2020_and_older_australia['date'] = joined_tweet_and_info_2020_and_older_australia['created_at'].apply(tweet_date_to_datetime_words)\n",
    "joined_tweet_and_info_2020_and_older_australia['year'] = joined_tweet_and_info_2020_and_older_australia['date'].dt.year \n",
    "\n",
    "\n",
    "# creating one large dataset from 2020 and 2021:\n",
    "\n",
    "joined_tweet_and_info_2020_and_older_australia_col_selcted = joined_tweet_and_info_2020_and_older_australia[['text','cleaned_text','name','party','country','date','year','id','uid','lang','retweet_screen_name','user_followers_count']].copy()\n",
    "\n",
    "joined_tweet_and_info_2021_australia_col_selcted = joined_tweet_and_info_2021_australia[['text','cleaned_text','name','party','country','date','year','id','uid','lang','retweet_screen_name','user_followers_count']].copy()\n",
    "\n",
    "full_tweet_dataset_politicians_australia = pd.concat([joined_tweet_and_info_2020_and_older_australia_col_selcted,\n",
    "                                            joined_tweet_and_info_2021_australia_col_selcted],\n",
    "                                            ignore_index= True)\n",
    "\n",
    "# adding climate_related column that will later be overwritten by classifier predictions\n",
    "full_tweet_dataset_politicians_australia['climate_related'] = 1\n",
    "\n",
    "\n",
    "# only select tweets longer than 3 words:\n",
    "full_tweet_dataset_politicians_australia['cleaned_text_word_count'] = full_tweet_dataset_politicians_australia['cleaned_text'].apply(word_count).astype('int')\n",
    "\n",
    "full_tweet_dataset_politicians_australia = full_tweet_dataset_politicians_australia[full_tweet_dataset_politicians_australia['cleaned_text_word_count'] >= 4]\n",
    "full_tweet_dataset_politicians_australia.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Australian Labor Party', 'The Nationals',\n",
       "       'Liberal Party of Australia', 'Independent', 'Nick Xenophon Team',\n",
       "       'Australian Greens', \"Katter's Australian Party\",\n",
       "       \"Pauline Hanson's One Nation\", \"Derryn Hinch's Justice Party\",\n",
       "       'Liberal National Party of Queensland'], dtype=object)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_tweet_dataset_politicians_australia['party'].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "timeseries follower counts long datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "follower_count_selected_politicians = combined_df_selected_polit[['author.name','author.public_metrics.followers_count']].groupby([combined_df_selected_polit.date.dt.to_period(\"M\"),'author.name']).mean().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "follower_count_selected_politicians.to_csv(\"./follower_timeseries/followers_timeseries_selected_pol.csv\", sep = \";\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "us_politicians_follower_count = full_tweet_dataset_politicians[['name','user_followers_count']].groupby([full_tweet_dataset_politicians.date.dt.to_period(\"M\"),'name']).mean().reset_index()\n",
    "uk_politicians_follower_count = full_tweet_dataset_politicians_uk[['name','user_followers_count']].groupby([full_tweet_dataset_politicians_uk.date.dt.to_period(\"M\"),'name']).mean().reset_index()\n",
    "australia_politicians_follower_count = full_tweet_dataset_politicians_australia[['name','user_followers_count']].groupby([full_tweet_dataset_politicians_australia.date.dt.to_period(\"M\"),'name']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "us_politicians_follower_count.to_csv(\"./follower_timeseries/followers_timeseries_us_pol.csv\", index=False)\n",
    "uk_politicians_follower_count.to_csv(\"./follower_timeseries/followers_timeseries_uk_pol.csv\", index=False)\n",
    "australia_politicians_follower_count.to_csv(\"./follower_timeseries/followers_timeseries_australia_pol.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Exporting the golden label climate classifier dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating a golden label dataset (sampling 2000 and manually labelling them in Label-Studio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_labels_dataset_politicians = full_tweet_dataset_politicians.sample(2000)\n",
    "golden_labels_dataset_politicians['climate_related_human_labelled'] = '-'\n",
    "golden_labels_dataset_politicians.to_csv('tweets/politicians/rehydrated/golden_labels_dataset_politicians.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "665796ea3363072d3a6057ac2fdbe3c4fcb0d17a4b92295d9707f78e9c46c0af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
